\documentclass{article}
\usepackage{amsmath}
\usepackage[margin=0.5in]{geometry}
\usepackage{amssymb,amscd,graphicx}
\usepackage{epsfig}
\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage{color}
\usepackage[]{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\bibliographystyle{unsrt}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{epsfig}  		% For postscript
%\usepackage{epic,eepic}       % For epic and eepic output from xfig
\renewcommand{\thesection}{}  % toc dispaly

\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newcommand{\ds}{\displaystyle}
\newcommand{\ul}{\underline}

\title{Applied Linear Algebra Notes, Fall 2021}
\date
\Large
\begin{document}
\maketitle
\large


\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fun Stuff}
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{enumerate}
\item Feynman Method: \url{https://www.youtube.com/watch?v=FrNqSLPaZLc}
\item Bad math writing: \url{https://lionacademytutors.com/wp-content/uploads/2016/10/sat-math-section.jpg}
\item Google AI experiments: \url{https://experiments.withgoogle.com/ai}
\item Babylonian tablet: \url{https://www.maa.org/press/periodicals/convergence/the-best-known-old-babylonian-tablet}
\item Parabola in real world: \url{https://en.wikipedia.org/wiki/Parabola#Parabolas_in_the_physical_world}
\item Parabolic death ray: \url{https://www.youtube.com/watch?v=TtzRAjW6KO0}
\item Parabolic solar power: \url{https://www.youtube.com/watch?v=LMWIgwvbrcM}
\item Robots: \url{https://www.youtube.com/watch?v=mT3vfSQePcs}, riding bike, kicked dog, cheetah, backflip, box hockey stick
\item Cat or dog: \url{https://www.datasciencecentral.com/profiles/blogs/dogs-vs-cats-image-classification-with-deep-learning-using}
\item History of logarithm: \url{https://en.wikipedia.org/wiki/History_of_logarithms}
\item Log transformation: \url{https://en.wikipedia.org/wiki/Data_transformation_(statistics)}
\item Log plot and population: \url{https://www.google.com/publicdata/explore?ds=kf7tgg1uo9ude_&met_y=population&hl=en&dl=en#!ctype=l&strail=false&bcs=d&nselm=h&met_y=population&scale_y=lin&ind_y=false&rdim=country&idim=state:12000:06000:48000&ifdim=country&hl=en_US&dl=en&ind=false} 
\item Yelp and NLP: \url{https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb} \url{https://www.yelp.com/dataset/challenge}
\item Polynomials and splines: \url{https://www.youtube.com/watch?v=O0kyDKu8K-k}, Yoda / matlab, \url{https://www.google.com/search?q=pixar+animation+math+spline&espv=2&source=lnms&tbm=isch&sa=X&ved=0ahUKEwj474fQja7TAhUB3YMKHY8nBGYQ_AUIBigB&biw=1527&bih=873#tbm=isch&q=pixar+animation+mesh+spline}, \url{http://graphics.pixar.com/library/}
\item Polynomials and pi/taylor series: Matlab/machin \url{https://en.wikipedia.org/wiki/Chronology_of_computation_of_%CF%80} 
\url{https://en.wikipedia.org/wiki/Approximations_of_%CF%80#Machin-like_formula}
\url{https://en.wikipedia.org/wiki/William_Shanks}
\item Deepfake: face \url{https://www.youtube.com/watch?v=ohmajJTcpNk} \\
dancing \url{https://www.youtube.com/watch?v=PCBTZh41Ris}
\item Pi digit calculations: \url{https://en.wikipedia.org/wiki/Chronology_of_computation_of_%CF%80}, poor shanks...\url{https://en.wikipedia.org/wiki/William_Shanks}
\end{enumerate}


\section{Course Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%
\item Syllabus highlights
\begin{enumerate}
\item Grades: 
\begin{enumerate}
\item Know the expectation / what you are getting into.
\item 15perc A (excellent), 35perc B (good), 35perc C (satisfactory),10perc D (passing), some F (failing)
\item Expect lower grades than you are used to. I was a student once upon a time. I know what it's like to give some effort in a class and still get an A/B. Night before study, good enough? 
\item Turn in an exam / project. Did you do good work?
\item Many will start off doing good / satisfactory work. Improve to something more. C is not the worst thing in existence. These letters say nothing of your capability. 
\end{enumerate}
\item What does good mean? Good means good. Good job! Excellent means you showed some flair.
\item Expect: More work, more expectation on good writing.
\item Math is a challenging subject. Not a natural thing to think or write in. It takes work and practice to be better. My goal is to train you to be better and give you ideas of where it can go.
\item Fact that you are here shows you are smart and capable. Your goal should be to improve. 
\item Why do I do this? I do it out of respect for you. You are smart enough. I want you to gain something valuable here. I wouldn't do this job if I didn't think you were gaining something of value.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%
\item Grand scheme of things. Where does this class sit inside all of mathematics.
%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate}
\item Basics: Algebra, arithmetic.
\item First steps: Geometry, functions. (us now)
\item Calculus: Math of change / infinity.
\item Linear algebra: Math of vectors. Anything with finite representation. Invention of computers fueled this one. Gateway to real math / applications.
\item Applied math. Any application you want. Physics, finance, marketing, material science, CFD, sports. 
\item Abstract math. Create your own world of ideas. Number theory, analysis, algebra, topology, more. 
\end{enumerate}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Day 1}

\begin{enumerate}
\item 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Chapter 1: Linear Equations in Linear Algebra} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{1.1 Systems of linear equations}

\begin{enumerate}

%%%%%%%%%%%%%%%%%%%
\item Definition: A \emph{linear equation} is of the form
\[
a_1 x_1 + a_2 x_2 + \dots + a_n x_n = b
\]
where $x_i$ are unknown variables with $a_i$ known constant coefficients and $b$ known constant. Only powers of 1 per variable. No other products or quotients.

%%%%%%%%%%%%%%%%%%%
\item Fundamental problem of linear algebra:
\begin{itemize}
\item Solve a system of linear equations (rich theory can completely study).
\item Key questions: Existence and uniqueness.
\end{itemize}

%%%%%%%%%%%%%%%%%%%
\item Familiar example, new ideas.
\begin{enumerate}
\item Solve for $x$ and $y$.
\[
\begin{cases}
2x-y=0 \\
-x+2y=3
\end{cases}
\]
Linear equations, graphs are lines in 2d. 

%%%%%%%%%%%%%%%%%%%
\item Three perspectives of this class:
\begin{itemize}
\item Row picture (familiar)
\item Column picture (new)
\item Matrix representation (maybe new)
\end{itemize}

\item Row picture: 
\begin{itemize}
\item Graph in $xy$-plane. Solution is intersection of two lines. How to find? Substitute or elimination.

\item In general, can see thee possibilities: Unique solution (lines differ in slope), infinite solutions (2 lines overlap), no solution (2 parallel non intersecting lines). No solution is called \emph{inconsistent}. One or infinite many solutions called \emph{consistent}.
\end{itemize}

%%%%%%%%%%%%%%%%%%%
\item Column picture: Vector representation
\begin{itemize}
\item Remind of 2D vector geometry, scalar multiplication, vector addition, graph, and linear combination. 

\item Rewrite in vector form. How to think of this? What linear combination of column vectors $\vec{v}_1$ and $\vec{v_2}$ result in vector $\vec{b}$? Draw in the plane and sketch solution. 

\item Verify that solutions $x=1, y=2$ from before work.

\item Again, three possibilities. What are the vector analogies regarding column vectors and RHS vector?

\item Generalize: If we change the RHS vector, will we always have a solution? In this case yes since $\vec{v}_1$ and $\vec{v_2}$ span $\mathbb{R}^2$. Change for parallel column vectors to see not always. 
\end{itemize}

%%%%%%%%%%%%%%%%%%%
\item Matrix representation: 
\begin{itemize}
\item Rewrite as coefficient matrix times unknown vector equal a RHS vector. 

\item Notation: Note text uses bold face letters for vectors.
\[
A, \quad \vec{x}, \quad \vec{b}
\]

\item Can also write short hand as an augmented matrix.

\item Solve using the same elimination strategy as with linear equations. Think of this as a computational view. Next section covers this.

\item Matrix $A$ can be thought of as an operator on solution vector $\vec{x}$ with resulting vector $\vec{b}$. Studying this linear system equations to studying properties of matrix $A$. 
\end{itemize}

\end{enumerate}


%%%%%%%%%%%%%%%%%%%
\item Higher dimensions:
\begin{enumerate}

\item 3 equations, 3 unknowns:
\[
\begin{cases}
x+2y+3z = 5 \\
2x+5y+2z = 7 \\
6x-3y+z = -2
\end{cases}
\]

%%%%%%%%%%%
\item Row picture
\begin{itemize}
\item Ask graph of each linear equation. Graph in Geogebra 3d to see. Can anyone solve? Plot solution point as well.

\item Again 3 cases here, but a bit richer. 1 solution, infinite solutions (plane or line of intersection), no solution (2 planes parallel but not the same). 

\item Solve by row reduction and backwards substitution. Goal is to replace system with equivalent, though simpler system. Summarize 3 elementary row operations (swap, scale, replace with row plus multiple of another). Why bother swap or scale? Take advantage of zeros and nice numbers. Computers care for high dimension to avoid roundoff error. Mention could eliminate all the way to Gauss Jordan form.

\end{itemize}

%%%%%%%%%%%
\item Column picture: Linear combination of three vectors giving RHS vector. Use Geogebra 3d again. Again, think of three cases. Key is all three vectors are linearly independent.

%%%%%%%%%%%
\item Matrix picture: Easy to write down? Now what?
\begin{itemize}
\item Can see columns of $A$ are column vectors. 
\item What about row vectors? Will develop this.
\item Augmented matrix. Algorithm in next section.
\end{itemize}

%%%%%%%%%%%%%%%%%%
\item Advantages / disadvantages of each picture: Combined they offer a complete theory.
\begin{itemize}
\item Row picture: Lots of info and intuition, cannot extend beyond 3d, will think in analogies.
\item Column picture: Easy to extend, hard to solve, lots of info and intuition.
\item Easy to adapt as algorithm, little intuition.
\end{itemize}

\end{enumerate}

%%%%%%%%%%%%%%%%%%%
\item Homework: 3, 7, 13, 18, 19, 23, 25, 33, 34

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{1.2 Row reduction and echelon form}


\begin{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%
\item 2 algorithms for solving linear systems of equations: 
\begin{itemize}
\item Gaussian elimination and backwards substitution (saw last time).
\item Gauss-Jordan elimination.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%
\item Example, $2 \times 2$: Solve the system using equation form.
\[
\begin{cases}
x - 2y = 1 ~(R_1) \\
3x + 2y = 11 ~ (R_2)
\end{cases}
\]

\begin{enumerate}
%%%%%%%%%%%%
\item Use the same forward reduction and back substitution idea as in last section.
\[
R_2 \rightarrow -3R_1 + R_2
\]
Check solution works. Recall 3 elementary row operations.

%%%%%%%%%%%%%%
\item Generalize: Use augmented matrix and aim towards a standard form.
\begin{itemize}
\item Row echelon form (GE)
\[
\left[
\begin{array} {cc|c}
1 & -2 & 1 \\
3 & 2 & 11
\end{array}
\right]
\rightarrow
\left[
\begin{array} {cc|c}
1 & -2 & 1 \\
0 & 8 & 8
\end{array}
\right]
\]
\item Reduced row echelon form (G-JE)
\[
\left[
\begin{array} {cc|c}
1 & -2 & 1 \\
3 & 2 & 11
\end{array}
\right]
\rightarrow
\left[
\begin{array} {cc|c}
1 & 0 & 3 \\
0 & 1 & 1
\end{array}
\right]
\]
\item Pivot entries correspond to locations of 1's in RREF. Pivot columns are columns which contain a pivot entry.
\item Note, for any matrix REF is not unique but RREF is. Will prove the latter later.
\end{itemize}

%%%%%%%%%%%%%%
\item What if...
\begin{itemize}

%%%%%%%%%%%%%%%%
\item No solution:
\[
\left[
\begin{array} {cc|c}
1 & -2 & 1 \\
3 & -6 & 11
\end{array}
\right]
\rightarrow
\left[
\begin{array} {cc|c}
1 & -2 & 1 \\
0 & 0 & 8
\end{array}
\right]
\]

%%%%%%%%%%%%%%%%%%%
\item Infinitely many solutions:
\[
\left[
\begin{array} {cc|c}
1 & -2 & 1 \\
3 & -6 & 3
\end{array}
\right]
\rightarrow
\left[
\begin{array} {cc|c}
1 & -2 & 1 \\
0 & 0 & 0
\end{array}
\right]
\]
Here $y$ is a free variable and all solutions are
\[
\begin{cases}
x = 1+2y \\
y \text{ free}
\end{cases}
\]
or written parametrically as
\[
\begin{cases}
x = 1+2t \\
y = t
\end{cases}
\]
for parameter $t$.
\end{itemize}

\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%
\item Example: Higher dimension, try on own:
\[
\begin{cases}
2x+4y-2z = 2 \\
4x+9y-3z = 8 \\
-2x-3y+7z = 10
\end{cases}
\]
REF and backwards sub vs RREF.

%%%%%%%%%%%%%%%%%%%%%%%%%
\item Homework: 1, 3, 5, 7, 11, 13, 15, 17, 21, 23, 33-34 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{1.3 Vector equations}


\begin{enumerate}

%%%%%%%%%%%%
\item 3 view of linear algebra:
\begin{itemize}
\item Equation (row picture)
\item Matrix 
\item Vector (column picture): This section, this is where we get geometric reasoning with math rigor.
\end{itemize}

%%%%%%%%%%%%
\item Definition: The vector space $\mathbb{R}^n$ consists of all column vectors $\vec{u}$ with $n$ real valued components.
\begin{itemize}
\item Notation: $\vec{u} = [u_1, u_2, \dots, u_n]^T$, each entry is called a component.
\item Special case: $\vec{0}$.
\end{itemize}

%%%%%%%%%%%%
\item Examples: Geometry of vectors, imagine displacement.
\begin{itemize}
\item $vec{u} = [1,2]^T \in \mathbb{R}^2$. Note not the same as (1,2). Vectors are location independent. Other examples in 4 quadrants. Sad zero vector.
\item $vec{u} = [-3,1,2]^T \in \mathbb{R}^3$
\end{itemize}

%%%%%%%%%%%%
\item Definitions: Vector operations
\begin{itemize}
\item Addition: $\vec{u} + \vec{v} = [u_1+v_1, \dots, u_n+v_n]^T$ in $\mathbb{R}^n$. Note need vectors of same length.
\item Scalar multiplication: $c \vec{u} = [c u_1, \dots, c u_n]^T$ for scalar $c$.
\item Subtraction (triangular law): $\vec{u}-\vec{v}$
\item Bonus (dot product to compare direction, more later): $\vec{u} \cdot \vec{v}$
\item Bonus (norm or length, more later): $\| \vec{u} \|_n = \sqrt{u_1^2 + \dots + u_n^2}$
\end{itemize}

%%%%%%%%%%%%
\item Examples: $\vec{u} = [1,2]^T, \vec{v} = [3,1]^T$
\begin{itemize}
\item $2\vec{u}, -\vec{u}, 4\vec{u}, 0\vec{u}, c\vec{u}$, set of all scalar multiples results in a line (rescaling gives name to scalar)
\item $\vec{u}+\vec{v}, \vec{v}+\vec{u}$ (Parallelogram law)
\item $\vec{u}-\vec{v} = \vec{u}+(-\vec{v})$ (Triangular law)
\end{itemize}

%%%%%%%%%%%%
\item Theorem (these mirror familiar algebraic properties, some proofs in HW): For all $\vec{u}, \vec{v} \in \mathbb{R}^n$ and scalars
\begin{enumerate}
\item $\vec{u}+\vec{v} = \vec{v}+\vec{u}$ (Commutative)
\item $(\vec{u}+\vec{v})+\vec{w} = \vec{u}+(\vec{v}+\vec{w})$ (Associative)
\item $\vec{u}+\vec{0}=\vec{u}$ (Identity)
\item $\vec{u} + (-\vec{u}) = \vec{0}$ for $-\vec{u} = (-1)\vec{u}$ (Inverse)
\item $c(\vec{u}+\vec{v}) = c\vec{u}+c\vec{v}$ (Distribution)
\item $(c+d)\vec{u} = c\vec{u}+d\vec{u}$ (Distribution)
\item $c(d\vec{u}) = (cd)\vec{u}$ (Compatibility)
\item $1\vec{u} = \vec{u}$ (Identity)	
\end{enumerate}

%%%%%%%%%%%%
\item Definition (the linear of linear algebra): Vector $\vec{y} \in \mathbb{R}^n$ is a linear combination of vectors $\vec{v}_1, \vec{v}_2, \dots, \vec{v}_n$ if there exists scalars $c_1, \dots , c_n$ (called weights) such that 
\[
\vec{y} = c_1 \vec{v}_1 + \dots + c_n\vec{v}_n
\]

%%%%%%%%%%%%
\item Example (Vector equation): Show that $\vec{b} = [3,1,-1]^T$ is a linear combination of vectors $\vec{a_1}=[2,0,-1]^T$ and $\vec{a_2} = [-1,1,1]^T$.
\begin{itemize}
\item This is equivalent to solving a linear system via GE.
\item Geogebra and geometric interpretation.
\item Is the same true for any $\vec{b}$? No, only if it lies in the plane generated by all linear combinations of $\vec{a_1}$ and $\vec{a_2}$. Consider a $\vec{b}$ which does not. 
\end{itemize}


%%%%%%%%%%%%
\item Definition: The collection of all linear combinations of $\vec{v_1}, \dots, \vec{v_p} \in \mathbb{R}^n$ is called the Span$\{\vec{v_1}, \dots, \vec{v_p} \}$ and is a subset of $\mathbb{R}^n$.

%%%%%%%%%%%%
\item Homework: 1, 3, 5, 7, 9, 11, 13, 15, 17, 21, 23, 27

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{1.4 The matrix equation $A \vec{x} = \vec{b}$}

\begin{enumerate}

%%%%%%%%%%%%
\item 3 views of linear algebra:
\begin{itemize}
\item Row picture (lines and planes, done)
\item Column picture (vectors, done)
\item Matrix picture (now, idea is to capture linear combination as an operation)
\end{itemize}

%%%%%%%%%%%%
\item Definition: For $A$ a $m \times n$ matrix with columns $\vec{a_1}, \dots, \vec{a_n}$ and $\vec{x} \in \mathbb{R}^n$, the produce $A\vec{x}$ is the linear combination of the columns of $A$ with weights as entries in $\vec{x}$. That is,
\[
A \vec{x} =
[\vec{a_1} \dots \vec{a_n}]
\left[ 
\begin{array}{c}
x_1 \\
\vdots \\
x_n
\end{array}
\right]
= x_1 \vec{a_1} + \dots x_n \vec{a_n}
\]
Note, the number of columns in $A$ must match the number of entries of $\vec{x}$. 

%%%%%%%%%%%%
\item Example: Multiply a random $A_{2 \times 3}$ matrix by a $\vec{x}_{3 \times 1}$ vector. 

3 linear algebra POVs are here. For general $\vec{x}$, write 
\begin{itemize}
\item 2 equations (planes, geometry)
\item Linear combinations of 3 vectors (vectors, geometry)
\item Matrix equation $A \vec{x} = \vec{b}$ (operation on a vector, similar to idea of function). Important question is given $A$, can we solve $A \vec{x} = \vec{b}$ for any RHS vector $\vec{b}$. 
\end{itemize}
We will readily switch between these views to gain insight and perspective.

%%%%%%%%%%%%
\item Example (entry-wise matrix multiplication): Multiply a random $A_{3 \times 3}$ matrix by a $\vec{x}_{3 \times 1}$ vector.
\begin{itemize}
\item Linear combination of 3 row vectors. Important concept.
\item Dot product of rows and $\vec{x}$. This version is more convenient for hand calculation.
\end{itemize}
Replace $A$ with identity matrix $I_{3 \times 3}$ and ask them to guess result. 

%%%%%%%%%%%%
\item Theorem (linearity of matrix multiplication): For matrix $A$ $m \times n$, vectors $\vec{u}, \vec{v}$ $n \times 1$, and scalar $c$, we have
\begin{enumerate}
\item $A(\vec{u} + \vec{v}) = A\vec{u} + A\vec{v}$ (distributive)
\item $A(c\vec{u}) = c(A\vec{u})$ (associative)
\end{enumerate}
Proof (of (a), $n=3$ case, (b) in text): All we need is the corresponding result from vectors in previous section.
\begin{align*}
A(\vec{u} + \vec{v})
&= A \left[
\begin{array}{c}
u_1+v_1 \\
u_2+v_2 \\
u_3+v_3
\end{array} \right]\\
&= (u_1+v_1) \vec{a_1} + (u_2+v_2) \vec{a_2} + (u_3+v_3) \vec{a_3} \\
&= (u_1 \vec{a_1} + u_2 \vec{a_2} + u_3 \vec{a_3})
+ (v_1 \vec{a_1} + v_2 \vec{a_2} + v_3 \vec{a_3})\\
&= A\vec{u} + A\vec{v}
\end{align*}

%%%%%%%%%%%%
\item Theorem (big result for entire course, will grow this list): For $A$ a $m\times n$ matrix, the following statements are either all true or all false.
\begin{enumerate}
\item For each $\vec{b} \in \mathbb{R}^n$, equation $A\vec{x}=\vec{b}$ has a solution.
\item Each $\vec{b} \in \mathbb{R}^n$ is a linear combination of the columns of $A$.
\item The columns of $A$ span $\mathbb{R}^m$.
\item $A$ has a pivot position in every row.
\end{enumerate} 

%%%%%%%%%%%%
\item Homework: 5, 7, 9, 11, 13, 15, 17, 23, 29, 30

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{1.5 Solution sets of linear equations}

\begin{enumerate}

%%%%%%%%%%%%%%%%%
\item We want to characterize solutions to a linear system of equations $A\vec{x} = \vec{b}$ for $A$ and $\vec{b}$ given and $\vec{x}$ unknown thru two perspectives:
\begin{itemize}
\item Geometrically (picture, intuition)
\item Explicitly (formula, practical)
\end{itemize}
Our approach will be to consider two related cases:
\begin{itemize}
\item Homogeneous linear system: $A\vec{x} = \vec{0}$
\item Nonhomogeneous linear system: $A\vec{x} = \vec{b}$
\end{itemize}

%%%%%%%%%%%%%%%%%
\item Homogeneous linear system: $A\vec{x} = \vec{0}$
\begin{enumerate}

%%%%%%%%%%
\item For any $A$, $\vec{x} = \vec{0}$ is always a solution (called the trivial solution). We seek nontrivial solutions $\vec{x} \neq \vec{0}$. Will there always be a nontrivial solution? Only if the GE solution has at least one free variable.

%%%%%%%%%%
\item Solve the homogeneous linear system:
\[
\left[
\begin{array}{ccc}
1 & 3 & -5 \\
1 & 4 & -8 \\
-3 & -7 & 9 
\end{array}
\right]
\vec{x} = 
\vec{0}
\]
Solving by GE gives $x_3$ a free variable with
\[
\vec{x} = 
x_3\left[
\begin{array}{ccc}
-4 \\
3 \\
1 
\end{array}
\right]
= x_3 \vec{v} = span \{ \vec{v} \}
\]
The set of these solutions are a line thru the origin parallel to $\vec{v}$. 


%%%%%%%%%%%%%%%%%
\item Change above example so three rows are multiples of eachother giving 2 free variables. 
\[
\left[
\begin{array}{ccc}
1 & 3 & -5 \\
1 & 3 & -5 \\
1 & 3 & -5 
\end{array}
\right]
\vec{x} = 
\vec{0}
\]
Solving by GE gives $x_2, x_3$ free variables with
\[
\vec{x} = 
\left[
\begin{array}{ccc}
-3x_2+5x_3 \\
x_2 \\
x_3 
\end{array}
\right]
= x_2 \vec{v_2} +  x_3 \vec{v_3} = span \{ \vec{v_2}, \vec{v_3} \}
\]
generating a plane thru the origin. View in Geogebra.

\end{enumerate}

%%%%%%%%%%%%%%%%%
\item Nonhomogenous linear system: $A\vec{x} = \vec{b}$
\begin{enumerate}
%%%%%%%%%%%%%
\item Example as from before:
\[
\left[
\begin{array}{ccc}
1 & 3 & -5 \\
1 & 4 & -8 \\
-3 & -7 & 9 
\end{array}
\right]
\vec{x} = 
\left[
\begin{array}{c}
4 \\
7 \\
6 
\end{array}
\right]
\]
gives
\[
\left[
\begin{array}{ccc|c}
1 & 3 & -5 & 4 \\
0 & 1 & -3 & 3 \\
0 & 0 & 0 & 0
\end{array}
\right]
\]
Again $x_3$ is free and we have
\[
\vec{x} = \left[
\begin{array}{c}
-4x_3 - 5 \\
3x_3 + 3 \\
x_3
\end{array}
\right]
=  \left[
\begin{array}{c}
-5 \\
3 \\
0
\end{array}
\right] + 
x_3 \left[
\begin{array}{c}
-4 \\
3 \\
1
\end{array}
\right]
= \vec{p} + x_3 \vec{v}
\]
for the same $\vec{v}$ as in the homogenous case.
Graph same lines as before but first shifted by vector $\vec{p}$ away from the origin.

%%%%%%%%%%%%%
\item Solution to nonhomogenous equation is the same as the homogenous case but translated.

%%%%%%%%%%%%%
\item Theorem: For $A\vec{x} = \vec{b}$ consistent and $\vec{p}$ a particular solution, then the solution set of all $A\vec{x} = \vec{b}$ is all vectors of the form
\[
w = \vec{p} + \vec{v_h}
\]
where $\vec{v_h}$ is any solution to the homogeneous equation $A\vec{x} = \vec{0}$. (sketch the plane case in $\mathbb{R}^3$)

\end{enumerate}

%%%%%%%%%%%%%%%%%
\item Homework: 1, 5, 7, 9, 11, 13, 17, 19, 21, 23, 27, 29, 31

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{1.6 Applications of linear systems}

\begin{enumerate}

\item Skip. Possible lab material.

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{1.7 Linear independence}

\begin{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item Homework: 9-20, 23-30

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{1.8 Introduction to the linear transformation}

\begin{enumerate}

\item Homework: 17-20, 25, 31

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{1.9 The matrix of a linear transformation}

\begin{enumerate}

\item Homework: 25-28, 31-34

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{1.10 Linear models in business, science, and engineering}

\begin{enumerate}

\item Homework:  

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Chapter 2: Matrix algebra} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{2.1 Matrix operations}

\begin{enumerate}

\item Homework: 13, 17-26

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{2.2 The inverse of a matrix}

\begin{enumerate}

\item Homework: 11-24, 35

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{2.3 Characterizations of invertible matrices}

\begin{enumerate}

\item Homework: 15-24

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{2.4 Partitioned matrices}

\begin{enumerate}

\item Homework: 1-10, 13, 14, 16

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{2.5 Matrix factorizations}

\begin{enumerate}

\item Homework: 22-26

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{2.6 The Leontief input-output model}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{2.7 Applications to computer graphics}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{2.8 Subspaces of $\mathbb{R}^n$}

\begin{enumerate}

\item Homework: 5-20, 23-26

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{2.9 Dimension and rank}

\begin{enumerate}

\item Homework: 9-16

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Chapter 3: Determinants} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{3.1 Introduction to determinants}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{3.2 Properties of determinants}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{3.3 Cramer's rule, volume, and linear transformations}

\begin{enumerate}

\item Homework: 

\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Chapter 4: Vector spaces} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{4.1 Vector spaces and subspaces}

\begin{enumerate}

\item Homework: 1-18, 23, 24

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{4.2 Null spaces, column spaces, and linear transformations}

\begin{enumerate}

\item Homework: 3-6, 17-26

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{4.3 Linearly independent sets, bases}

\begin{enumerate}

\item Homework: 21-25

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{4.4 Coordinate systems}

\begin{enumerate}

\item Homework: 25 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{4.5 The dimension of a vector space}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{4.6 Rank}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{4.7 Change of basis}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{4.8 Applications to difference equations}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{4.9 Applications to Markov chains}

\begin{enumerate}

\item Homework: 

\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Chapter 5: Eigenvalues and eigenvectors} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{5.1 Eigenvectors and eigenvalues}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{5.2 The characteristic equation}

\begin{enumerate}

\item Homework: 25, 27

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{5.3 Diagonalization}

\begin{enumerate}

\item Homework: 18

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{5.4 Eigenvectors and linear transformations}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{5.5 Complex eigenvalues}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{5.6 Discrete dynamical systems}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{5.7 Applications to differential equations}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{5.8 Iterative estimates to eigenvalues}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Chapter 6: Orthogonality and least squares} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{6.1 Inner product, length, and orthogonality}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{6.2 Orthogonal sets}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{6.3 Orthogonal projections}

\begin{enumerate}

\item Homework: 19, 20

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{6.4 The Gram-Schmidt process}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{6.5 Least-squares problems}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{6.6 Applications to linear models}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{6.7 Inner product spaces}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{6.8 Applications of inner product spaces}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Chapter 7: Symmetric matrices and quadratic forms} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{7.1 Diagonalization of symmetric matrices}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{7.2 Quadratic forms}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{7.3 Constrained optimization}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{7.4 The singular value decomposition}

\begin{enumerate}

\item Homework: 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{7.5 Applications to image processing and statistics}

\begin{enumerate}

\item Homework: 

\end{enumerate}

\end{document}