	\documentclass{article}

\usepackage[margin=0.75in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{hyperref}


\newcommand{\ds}{\displaystyle}
\newcommand{\ul}{\underline}

\title{Unsupervised Learning Notes}
\date
\Large
\begin{document}
\maketitle
\large


\tableofcontents
\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Introduction to unsupervised learning} 
\addcontentsline{toc}{section}{Introduction to unsupervised learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Idea of unsupervised learning}
\addcontentsline{toc}{subsection}{Idea of unsupervised learning}

\begin{enumerate}

\item Slideshow: What is AI, ML, data science, statistics, etc? How does it all fit? Cool diagrams.


\item Supervised learning vs unsupervised learning:
\begin{enumerate}
\item Supervised learning:
\begin{itemize}
\item Given labeled data, predict a target. Example is predicting a student's GPA from student data.
\item More formally, labeled training data is $(X, y)$ for $n$ samples with relationship
\[
y = f(X) + \epsilon.
\]
Goal is to approximate $f$ via $\hat{f}$. Call supervised because we know answers from our training data.
\item Basic example of linear regression with $p=1$ for student data. Assume $f$ is linear. Graph with data and best fit line $\hat{f}$. Write general $p$ variable case. Can be used for student intervention (early warning, effect of living on campus, attending class). 
\item Main goals are prediction and inference by understanding $f$ (feature importance, model fit and reliability).
\item Linear model is an assumption. Other models include decision trees, neural networks, support vector machine, and more.
\item Well understood area, clear ways to assess quality of results. ISLR text key reference.
\end{itemize}
\item Unsupervised learning:
\begin{itemize}
\item Unsupervised learning is a class of machine learning methods used to discover structure in data
\[
X = \{ x_1, x_2, \dots, x_n \}, \quad x_i \in \mathbb{R}^p \quad \text{(note vector notaiton)}
\] without labeled outcomes. Instead of predicting a known target, the goal is to explore, summarize, and reveal patterns that are intrinsic to the data.
\item Given unlabeled data $X$, find structure in the data. No prediction, no supervision. Learning by observation, not by example.
\item What types of students are there given hours studied per week and GPA? What variable combinations belong together (academic, engagement)? Unusual students? Goal is to better understand data.
\item Can be a stand alone analysis or can be used to compliment supervised learning.
\end{itemize}
\end{enumerate}

\item Core tasks of unsupervised learning:
\begin{enumerate}
\item Clustering: Group similar observations in the same cluster. K-means, hierarchical, dbscan
\item Dimension reduction: Reduce noise and multicolinearity, data viz, large to smaller data, data understanding. PCA, t-SNE, UMAP, SVD. Google tensorflow embedding projector.
\item Anomaly detection: Learn distribution of data to quantify outlier probabilities.
\end{enumerate}

\item Concrete examples of unsupervised learning:
\begin{enumerate}
\item Customer segmentation: Clustering
\begin{itemize}
\item Walmart data on spend average, frequency, mode, product mix, app use. 
\item No label such as budget shopper or family provider. Want to discover segments rather than predetermine behavior.
\item Each data point is a customer in high dimensions. 
\item Similarity means close distance. Scaling and choice of distance matters.
\item Possible clusters: Weekly family stockup, single essentials, deal hunters. These are business driven interpretations.
\item Actions: Store layout optimization, personal coupons, inventory planning, regional differences. Not aiming for individual predictions (as with supervised learning).
\item Google: Walmart customer segmentation
\end{itemize}
\item Spotify music genre: Dimension reduction
\begin{itemize}
\item Google: Spotify api dataset
\item Many automatic features, how to tell what genre?
\item Vectors are high dimension, but human perception is low dimension.
\item Can we compress data into low-dimensions?
\item Distance reflect song similarity.
\item Are there distinct groups of genres or continuous flow?
\item Goal is to make data more intelligible.
\end{itemize} 
\item Credit card fraud: Anomaly detection
\begin{itemize}
\item Each data point is a transaction (amount, time, source, location, recent freq, device).
\item Millions of these per day, tiny amount are fraud.
\item False positive is a problem.
\item Does this deviate from expected? Normal behavior is dense regions, but some deviance can naturally occur.
\item Distance metric determines deviation from normal.
\item Per customer (card?) normalization. 
\item Action may be to identify fraud patterns to catch more.
\end{itemize}
\end{enumerate}

\item Much more challenge than supervised learning. No simple goal. Results are subjective. Exploratory, descriptive, and hypothesis-generating rather than predictive.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Readings}
\addcontentsline{toc}{subsection}{Readings}

\begin{enumerate}
\item ISLR 2.1.4, Ch12 thru 12.1
\item HOUL Ch1
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Lab}
\addcontentsline{toc}{subsection}{lab}

\begin{enumerate}
\item EDA and data cleaning, DMCT Ch2 and Ch3
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Distance and similarity} 
\addcontentsline{toc}{section}{Distance and similarity}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Feature scaling and normalization}
\addcontentsline{toc}{subsection}{Feature scaling and normalization}

\begin{enumerate}
\item Features on a bigger scale dominate distance calculations. Spend vs weekly visit count for customer segmentation. Illustrate dollars vs thousands for spend on Euclidean distance.
\[
d_2(x,y) = \|x-y\|_2 = \sqrt{\sum (x_j-y_j)^2}
\]
Many ways to scale to prevent this issue.

\item Standardization ($z-$score scaling):
\begin{enumerate}
\item Definition: Shift by mean and divide by standard deviation.
\[
x_i' = \frac{x_i - \overline{x}}{s_x}
\]
where $s_x$ is the standard deviation of variable $x$. Remind of standard deviation calculation. Recall sample vs population notation. Draw distribution picture, two different shapes.
\[
s_x = \sqrt{\frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2}, \quad
\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i
\]
\item Results in mean 0, variance 1. Now on a standard normal distribution $N(0,1)$. Nice properties of a normal distribution for 68-95-99 percent confidence intervals. Preserves order and relative distribution.
\item Equal feature contribution, preserves relative difference.
\item Extensions include mean absolute deviation.
\end{enumerate}

\item Min-max scaling: 
\begin{enumerate}
\item Definition: Shift by min and divide by range.
\[
x_i' = \frac{x_i - \min(x)}{\max(x)-\min(x)}
\]
\item Guess what it does. Refer to mall data scaling age.
\item Result is in interval $[0,1]$. Preserves order but compresses extremes.
\item Good for bounded features. 
\end{enumerate}

\item Most of the time scale, can always revers scaling if you keep track of original stats. When scaling is NOT a good idea:
\begin{enumerate}
\item Feature units are meaningful, important, and want to keep for analysis.
\item Binary indicators (0/1), changes to discrete values but different values.
\item Counts with semantic meaning, such as a 1-10 satisfaction rating.
\item Ratios which are already normalized.
\end{enumerate}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Distance metrics and similarity}
\addcontentsline{toc}{subsection}{Distance metrics}

\begin{enumerate}
\item Distance is a choice we make which encodes different notions of similar. Many options.
\item Key properties are required: Mathematically called a metric in a metric space.
\begin{enumerate}
\item Positivity: $d(x,y) \geq 0$ for all $x,y$ and $d(x,y) = 0$ only if $y=x$.
\item Symmetry: $d(x,y) = d(y,x)$.
\item Triangle inequality: $d(x,z) \leq d(x,y) + d(y,z)$ for any $x,y,z$. Illustrate with pictures, $x$ and $z$ on the horizontal axis.
\end{enumerate}
\item Common distance metrics:
\begin{enumerate}
\item Euclidean distance ($\ell_2$ norm): For points $x, y \in \mathbb{R}^d$,
\[
d_2(x,y) = \|x-y\|_2 = \sqrt{\sum (x_i-y_i)^2}
\]
\begin{itemize}
\item Geometry: Straight-line distance, rotation invariant, penalizes large feature deviations heavily.
\item Assumes: Features are commensurate, spherical neighborhoods make sense
\item Example: Customers close if very same spend pattern and volume. (same shop and spend)
\end{itemize}

\item Manhattan distance ($\ell_1$ norm):
\[
d_1(x,y) = \|x-y\|_1 = \sum |x_i - y_i|
\]
\begin{itemize}
\item Geometry: City block distance, diamond shaped contours, less sensitive to large feature deviations.
\item Assumes: More robust to outliers.
\item Example: Customers close if same spend pattern and volume, some diff tolerated. (similar with occasional deviation allowed)
\end{itemize}

\item Supremum ($\ell_{\infty}$ norm):
\[
d_{\infty}(x,y) = \max \{|x_i-y_i| \}
\]
\begin{itemize}
\item Geometry: Largest difference only. Shapes are squares.
\item Example: Customers close if same spend pattern and volume, one big difference is a problem. 
\end{itemize}

\item These are all generalized by Minkowski distance:
\[
d(x,y) = \|x-y\|_r = \sqrt{\sum (x_i-y_i)^r} 
\]
Overall, Euclidean is the favorite, but there are good reasons to use others. Show animation in Desmos of different unit circles $x^r+y^r = 1$. $r=\infty$ case is the limit which calc 1 shows supremum norm is found. Show in 2D.
\end{enumerate}

\begin{enumerate}
\item Similarities: Note not a distance, but sometimes makes more sense.

\item Cosine similarity: 
\[
sim_{cos}(x,y) = \frac{x \cdot y}{\|x\| \|y\|} = \cos(\theta)
\]
\begin{itemize}
\item Geometry: measures cosine of angle between rather than distance, lives in $[-1,1]$, same direction is close to 1, orth and opposite direction, ignores scale. Turns our ordering agrees with Euclidean distance.
\item Assumes: Pattern rather than intensity.
\item Example: Customers close if same spend pattern but different spend volumes. (shop same regardless of spend)
\end{itemize}

\item Correlation: Sample Pearson correlation.
\[
sim_{cor}(x,y) = r = cor(x,y) =
\frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}
{\sqrt{\sum_{i=1}^n (x_i - \bar{x})^2}\;
 \sqrt{\sum_{i=1}^n (y_i - \bar{y})^2}}
\]
\begin{itemize}
\item Geometry: Measures linear relationship, $r\approx 1$ strong positive linear relationship, $r \approx -1$ strong negative, in between. Book picture vs Wikipedia picture.
\item Same as cosine similarity if data is normalized.
\end{itemize}

\end{enumerate}
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Curse of dimensionality}
\addcontentsline{toc}{subsection}{Curse of dimensionality}

\begin{enumerate}
\item In high dimensions, all points become almost equally far apart. Nearest points are almost same distance as farthest.
\item As dimension $d$ increases, 
\[
\frac{\max d(x,y) - \min d(x,y)}{\min d(x,y)} \rightarrow 0
\]
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Readings}
\addcontentsline{toc}{subsection}{Readings}

\begin{enumerate}
\item IDM Ch 2 Intro, 2.3.7, 2.4.1-2.4.5, 2.4.9-2.4.10 recommend all of Ch 2
\item DMCT Ch 2 Intro, 2.5.1, 2.3.1, 2.3.4, 2.3.7, 2.3.9 recommend all of Ch 2 except 2.6
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Lab}
\addcontentsline{toc}{subsection}{lab}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Clustering methods} 
\addcontentsline{toc}{section}{Clustering methods}

Main reference: DMCT chapter 9

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Intro to clustering}
\addcontentsline{toc}{subsection}{Intro to clustering}

\begin{enumerate}
\item What is cluster analysis?
\begin{enumerate}
\item Clustering groups data into clusters so that objects within a cluster are more similar to each other than to objects in other clusters, according to a chosen notion of similarity.
\item Clustering is an \emph{ill-posed} problem: there is no single correct solution. Results depend on modeling assumptions, similarity definitions, and analysis goals. There is no universal notion of “true” clusters.
\item Similarity (or dissimilarity) is typically defined through a distance or proximity measure.
\item Different clustering methods reflect different assumptions about data structure (e.g., partitioning, hierarchical, density-based, grid-based). Some methods that optimize an explicit objective (k-means) while others identify structure without a global objective (hierarchical, DBSCAN).
\item Clustering quality can be assessed in multiple ways, including internal criteria, external validation, stability, and interpretability.
\item Ongoing research focuses on scalability, high-dimensional settings where distance metrics break, complex cluster shapes, and diverse data types (e.g., text, images).
\end{enumerate}

\item Desiderata for clustering methods:
\begin{enumerate}
\item Ability to handle different data types (numeric, categorical, mixed)
\item Ability to detect non-spherical or non-convex clusters
\item Robustness to noise and outliers
\item Scalability to large datasets
\item Ability to incorporate constraints or side information
\item Results that are interpretable and actionable
\end{enumerate}

\item Types of clustering methods
\begin{enumerate}

\item Partitioning methods (customer segmentation, geog segm)
\begin{itemize}
\item Assume clusters are compact, well-separated, and cover all data points.
\item Partition $n$ objects into $k$ non-overlapping clusters.
\item Typically distance-based and solved via iterative optimization.
\item Sensitive to initialization, distance choice, and cluster shape assumptions.
\end{itemize}

\item Hierarchical methods (fish in Mississippi, species, genus, family, order, class, phylum, kingdom)
\begin{itemize}
\item Assume nested cluster structure is meaningful.
\item Produce a hierarchy of clusters represented as a tree.
\item Agglomerative (bottom-up) or divisive (top-down) approaches.
\item Once a merge or split occurs, it cannot be undone.
\end{itemize}

\item Density-based methods (social data with natural crowding, maybe no fixed $k$ or hierarchy)
\begin{itemize}
\item Assume clusters correspond to regions of high data density separated by low-density regions.
\item Clusters are grown based on neighborhood density criteria.
\item Naturally identify outliers and allow arbitrary cluster shapes.
\end{itemize}

\item Key challenges and limitations:
\begin{itemize}
\item Evaluation is inherently difficult due to lack of ground truth; metrics often encode the same assumptions as the algorithm.
\item Clustering is exploratory rather than confirmatory.
\item Domain knowledge plays a central role (feature selection, scaling, similarity choice, interpretation, clustering alg choice).
\item Clustering is not classification, causal inference, or discovery of objective real-world categories; results should not be over-interpreted.
\end{itemize}

\end{enumerate}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Partitioning methods: K-means and k-medoids clustering}
\addcontentsline{toc}{subsection}{Partitioning methods: K-means and k-mediods clustering}

\begin{enumerate}

\item Big picture:
\begin{enumerate}
\item Given data $D = \{x_1,\dots,x_n\} \subset \mathbb{R}^p$, partition into $k$ disjoint clusters $C_1,\dots,C_k$, $C_i \cap C_j = \emptyset$.
\item Each point belongs to exactly one cluster, and each cluster is summarized by a single representative (center).
\item Clustering defined via optimization of an objective function called within-cluster loss.
\end{enumerate}

\item Key assumptions:
\begin{enumerate}
\item A meaningful distance $d(x,y)$ exists (eg Euclidean distance).
\item Clusters are compact and well-separated in the chosen metric.
\item Cluster centers summarize cluster geometry.
\item All data belong to some cluster (no noise model).
\item $k$ is fixed and meaningful.
\end{enumerate}

\item $k$-means: A centroid-based technique
\begin{enumerate}
\item Cluster center: centroid (mean) $c_i \in \mathbb{R}^p$.

\item Objective function to minimize:
\[
\min_{C_1,\dots,C_k} \sum_{i=1}^k \sum_{x_j \in C_i} d(x_j , c_i)^2,
\quad
c_i = \frac{1}{|C_i|}\sum_{x_j \in C_i} x_j.
\]
where the distance metric is Euclidean distance (almost always)
\[
d(x,y) = \|x-y\| = \sqrt{(x_1-y_1)^2 + \dots + (x_p-y_p)^2}.
\]
The centroid represents a hypothetical average cluster member (customer).
\item The within cluster variance is the sum of square errors:
\[
WCSS = \sum_{i=1}^k \sum_{x_j \in C_i} d(x_j , c_i)^2
\]

\item Algorithm (Lloyd’s algorithm):
\begin{itemize}
\item This optimization problem is computationally expensive (NP-hard), many ways to select $k$ clusters of $n$ data points. A basic iterative algorithm is used instead.
\item Initialize $c_1,\dots,c_k$ as $k$ random objects from $D$.
\item Cluster assignment step: Belong to nearest centroid.
\[
x_j \mapsto \arg\min_i \|x_j - c_i \|^2.
\]
\item Centroid update step: Average of cluster members.
\[
c_i \leftarrow \frac{1}{|C_i|}\sum_{x_j \in C_i} x_j.
\]
\item Iterate until assignments stabilize.
\end{itemize}

\item Each step decreases the objective, converges to a local minimum, may not be global minimum.
\item Implies Voronoi partition of the feature space, boundaries between clusters where two centroids are equidistant, only depends on centroid not data distribution.
\item Implicit assumptions: spherical clusters, equal variance, Euclidean geometry.
\item Variations of $k$-means involved different distance metrics, smart centroid initialization, and centroid calculation strategies, $k$-modes for nominal data, groupings of data called microclusters.
\end{enumerate}

\item $k$-medoids: A representative object-based technique
\begin{enumerate}
\item Motivation: $k$-means is sensitive to outliers when centroids (means) are calculated.
\item Cluster center: medoid $o_i \in {x_1,\dots,x_n}$. Centroid is now a data point. Also called a representative object.
\item Objective function to minimize: Note that medoids $o$ must be data points, so computing new centers is not obvious.
\[
\sum_{i=1}^k \sum_{x_j \in C_i} d(x_j , o_i)
\]
Distance $d(\cdot,\cdot)$ need not be Euclidean. Note the lack of squared distance.
\item More robust to outliers (no averaging).
\item Example algorithm: Partitioning around medoids PAM
\begin{itemize} 
\item Random initial medoids. Assign cluster membership. Brute force check all updated medoids per assigned cluster to see which minimizes total distance error. Iterate.
\[
WCTD = \sum_{i=1}^k \sum_{x_j \in C_i} d(x_j , o_i)
\]
\item Modification for large data, clustering large applications (CLARA) considers random samples of the dataset.
\end{itemize}
\item Slower than $k$-means due to discrete optimization, especially for large $n$ and large $k$.
\end{enumerate}

\item $k$-means vs $k$-medoids (mathematical contrast)
\begin{itemize}
\item Continuous optimization (means) vs discrete optimization (medoids).
\item Squared Euclidean loss vs general metric loss.
\item Sensitive vs robust to outliers.
\item Fast gradient-like updates vs combinatorial search.
\end{itemize}

\item Practical issues:
\begin{itemize}
\item Objective function is typically non-convex $\Rightarrow$ multiple local minima. Repeat iterations can give different cluster solutions.
\item Initialization matters (e.g., random vs k-means++ which chooses initial clusters far apart).
\item Scaling changes the geometry of $|\cdot|$.
\item Choice of $k$ is a modeling decision, not a statistical estimate. If you don't know $k$, try many and compare results.
\end{itemize}

\item When partitioning methods work well
\begin{itemize}
\item Clusters roughly convex and isotropic (not stretched in a certain direction).
\item Moderate dimension with meaningful distances, otherwise dimension reduction needed.
\item Clear notion of “center.”
\item Need fast baseline clustering.
\end{itemize}

\item When they fail
\begin{itemize}
\item Non-convex or nested clusters.
\item Unequal cluster variances or densities.
\item Strong outliers (especially $k$-means).
\item High-dimensional distance concentration.
\end{itemize}

\item How to determine the right number of clusters? Many ways! Basic discussion here, more later.
\begin{enumerate}
\item Need $k$ in advance, but the right number of clusters is often ambiguous.
\item Good to balance compressibility (simplifying data) with accuracy (cluster meaning and usability).
\item An easy approach considers plotting WCSS (within cluster sum of squares) for $k=2,3,4,\dots,k$ and use the elbow method. Sharp bend says smaller improvements. Expect to decrease to 0. Subjective but something to go by.  Better(?) ways!
\end{enumerate}

\item Fun simulator: \url{https://clustering-visualizer.web.app/kmeans}

\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Hierarchical Methods}
\addcontentsline{toc}{subsection}{Hierarchical Methods}

\begin{enumerate}
\item Big picture:
\begin{enumerate}
\item Hierarchical clustering builds a nested sequence of partitions.
\item Output is a tree (dendrogram), not a single clustering.
\item Clusters exist at multiple resolutions (choices of $k$).
\item No single "best" number of clusters is assumed a priori.
\item 2 main types: Agglomerative (bottom-up) and divisive (top-down)
\end{enumerate}

\item Key assumptions:
\begin{enumerate}
\item Nested structure in the data is meaningful.
\item Pairwise dissimilarities capture relevant structure.
\item Early decisions (merges or splits) are trustworthy.
\item No noise model: all points participate in the hierarchy.
\end{enumerate}

\item Agglomerative hierarchical clustering:
\begin{enumerate}
\item Algorithm: Bottom-up procedure.
\begin{itemize}
\item Start with $n$ singleton clusters $\{x_1\}, \{x_2\}, \dots , \{x_n\}$ . Compute a pairwise distance matrix. 
\item Iterative merge the two closest clusters. The disimilarity between these two clusters indicates the height in the dedrogram where the fusion is places.
\item Computed updated pairwise inter-cluster disimlarities among the $n-1$ remaining clusters. Repeat merging.
\item Merge low in dendrogram means joined clusters were similar. High merge means clusters were dissimilar (separation).
\end{itemize}

\item Cluster-cluster distance (linkage)
\begin{itemize}
\item Requires a linkage function $D(C_a,C_b)$ for clusters $C_a, C_b$.
\item Common choices:
\[
\text{Single / Minimum / Nearest neighbor: } D(C_a,C_b) = \min_{x \in C_a, y \in C_b} d(x,y)
\]
Allows long, thin, winding, non-convex clusters. Connectivity matters, global compactness does not.
\[
\text{Complete / Maximum / Farthest neighbor: } D(C_a,C_b) = \max{x \in C_a, y \in C_b} d(x,y)
\]
Produces compact, spherical clusters and penalizes irregular shapes. Sensitive to outliers.
\[
\text{Average: } D(C_a,C_b) = \frac{1}{|C_a| |C_b|} \sum_{x \in C_a} \sum_{y \in C_b} d(x,y)
\]
\[
\text{Mean / Centroid: } D(C_a,C_b) = |m_a - m_b|
\]
Average and mean are compromise between single and complete linkage. 
\end{itemize}
\item Linkage choice encodes shape assumptions. Distance metric also important per above discussion (what similar should mean in application).
\end{enumerate}

\item Ward's method (variance-based linkage)
\begin{itemize}
\item Merge clusters that miminally increase total within-cluster variance.
\item Objective interpretation:
\begin{align*}
W (C_i, C_j) &= WCSS(C_i \cup C_j) -  WCSS(C_i) -  WCSS(C_j) \\
&= \sum_{\vec{x} \in C_i \cup C_j} d(\vec{x}, \vec{c_{ij}})^2 -  \sum_{\vec{x} \in C_i} d(\vec{x}, \vec{c_{i}})^2 -  \sum_{\vec{x} \in C_j} d(\vec{x}, \vec{c_{j}})^2
&= \frac{|C_i||C_j|}{|C_i|+|C_j|} \| \vec{c_i} - \vec{c_j} \|^2
\end{align*}
where $c$ denotes the clsuter centroid as in k-means. Note only for Euclidean distance for last step.
\item Closely related to k-means objective, though indirectly. Best of both worlds in a way.
\item Favors compact, spherical clusters.
\end{itemize}

\item Dendogram: Illustrate example with basic distance measures.
\begin{itemize}
\item Tree structure encoding merge order and merge distances.
\item Vertical height = dissimilarity at which merge occurs. Note near in the horizontal direction does not mean points/clusters are near. Good discussions in ISLR,
\item Cutting the tree at height $h$ induces a partition.
\item Different cuts correspond to different $k$.
\end{itemize}

\item How to decide cut?
\begin{itemize}
\item Unlike k-mean, HCA does not optimize a cost function such as WCSS. Other approaches are needed.
\item Dendogram viz is key. 
\item Elbow method of linkage distance vs merge step (A sharp increase in linkage distance = merging dissimilar clusters, cut just before the big jump)
\item WCSS works for Ward's linkage because it is inherently close to $k$-means.
\item Cophenetic Correlation Coefficient (CCC)
\begin{itemize}
\item The cophenetic correlation coefficient (CCC) is specific to hierarchical clustering and is often overlooked.
\item Idea: How well does the dendrogram preserve the original pairwise distances?
\[
\mathrm{CCC} = \mathrm{corr}\!\left( d_{ij}, \hat{d}_{ij} \right)
\]
where $d_{ij}$ is the original distance between points $i$ and $j$, and $\hat{d}_{ij}$ is the cophenetic distance, defined as the height at which points $i$ and $j$ merge in the dendrogram.
\item Interpretation: Values close to $1$ indicate that the hierarchical clustering preserves pairwise distances well. Low values indicate that the dendrogram substantially distorts the geometry of the data.
\item Uses: Comparing linkage methods (single, complete, average, Ward), and choosing an appropriate distance metric.
\end{itemize}
\end{itemize}

\item Divisive hierarchical clustering
\begin{itemize}
\item Top-down approach.
\item Start with all points in one cluster.
\item Recursively split clusters.
\item Less common due to computational cost.
\item Conceptually closer to repeated partitioning.
\end{itemize}

\item When hierarchical clustering works well
\begin{itemize}
\item Data have meaningful nested or multi-scale structure.
\item Moderate sample size.
\item Interest in relationships between clusters, not just assignments.
\end{itemize}

\item When it fails
\begin{itemize}
\item Large datasets (computational and memory cost).
\item Strong noise or chaining effects (single linkage).
\item Early incorrect merges propagate upward.
\item Noisy distance measurements.
\end{itemize}

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Density-based clustering: DBSCAN}
\addcontentsline{toc}{subsection}{Density-based spatial clustering of applications with noise (DBSCAN)}

\begin{enumerate}
\item Big picture:
\begin{enumerate}
\item DBSCAN = Density-based spatial clustering of applications with noise
\item Clusters are defined as regions of high point density separated by regions of low density.
\item Does not impose global geometry (no centroids, no partition).
\item Explicitly allows noise and outliers.
\item Number of clusters is determined by the data, not fixed in advanced.
\end{enumerate}

\item Key assumptions:
\begin{enumerate}
\item A meaningful distance metric exists.
\item Clusters correspond to dense regions in the metric space.
\item Density is approximately homogeneous within the clusters.
\item Low-density regions separate clusters.
\end{enumerate}

\item Parameters:
\begin{enumerate}
\item $\varepsilon > 0$ (radius parameter).
\item $\text{minPts} \in \mathbb{N}$ (minimum number of neighbors)
\end{enumerate}

\item Neighborhood definition:
\begin{enumerate}
\item $\varepsilon$-neighborhood of a point $x$:
\[
N_{\varepsilon} (x) = \{y : d(x,y) \leq \varepsilon \}
\]
\item $|N_{\varepsilon} (x)|$ gives the density of point $x$, including the point itself in this count.
\end{enumerate}

\item Point types: 
\begin{enumerate}
\item Core point: At least mnPts within a neighborhood of that point. Point $A$ in diagram if $MinPts \geq 7$. 
\[
|N_{\varepsilon} (x) | \geq \text{minPts}
\]
\item Border point: Not core, but falls in the neighborhood of a point (within the border), or possibly many points. Point $B$ in diagram.
\[
|N_{\varepsilon} (x) | < \text{minPts}, \quad \text{but } x \in N_{\varepsilon}(y) \text{ for some core point } y
\]
\item Noise point: $x$ is neither core nor border. Point $C$ in diagram.
\end{enumerate}

\item Density reachability:
\begin{enumerate}
\item Directly density-reachable:
\[
y \in N_{\varepsilon}(x), \quad x \text{ is a core point}
\]
\item Density-reachable: chain of directly density-reachable points.
\item Density-connected: two points reachable from a common core point.
\end{enumerate}

\item Cluster definition:
\begin{enumerate}
\item A cluster is a maximal set of density-connected points.
\item Noise points are not assigned to any cluster.
\end{enumerate}

\item Algorithm (conceptual):
\begin{enumerate}
\item Identify all core points by checking neighborhood density of all possible points.
\item Grow clusters by connecting density-reachable points into the same cluster.
\item Label remaining points as noise or border.
\end{enumerate}

\item Geometric consequences:
\begin{enumerate}
\item Can recover non-convex and arbitrarily shaped clusters.
\item No forced assignment of all points.
\item Cluster boundaries follow low-density regions.
\item No global partition space.
\end{enumerate}

\item Comparison to partitioning methods
\begin{enumerate}
\item No centroids or objective function.
\item No Voronoi geometry.
\item $k$ not specified.
\item Explicit noise handling.
\end{enumerate}

\item Sensitivity and limitations
\begin{enumerate}
\item Choice of $\varepsilon$ and minPts is critical.
\item Struggles with varying cluster densities.
\item Distance concentration in high dimensions degrades performance.
\item Sensitive to distance scaling.
\end{enumerate}

\item When DBSCAN works well
\begin{enumerate}
\item Clusters separated by low-density regions.
\item Non-spherical, irregular shapes.
\item Presence of noise or outliers.
\item Low-to-moderate dimensional data.
\end{enumerate}

\item When it fails
\begin{enumerate}
\item Clusters with significantly different densities.
\item High-dimensional data.
\item Data without clear density gaps.
\item Poorly chosen distance metric.
\end{enumerate}

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Comparison of k-means, hierarchical, and DBSCAN methods}
\addcontentsline{toc}{subsection}{Comparison of k-means, hierarchical, and DBSCAN methods}

\begin{enumerate}
\item Key idea: Clustering methods are not interchangeable algorithms. They encode fundamentally different notions of what a cluster is.

\item The question each method answers:
\begin{enumerate}
\item Partitioning (k-means and k-modes): Given $k$, how should I divide all points to minimize within-cluster dissimilarity?
\item Hierarchical clustering: How are points related across multiple scales of similarity?
\item DBSCAN: Which points belong to the same dense region, and which points are noise?
\end{enumerate}

\item Mathematics of the machines:
\begin{enumerate}
\item Partitioning: 
\begin{itemize}
\item Force the data into $k$ compact clusters by minimizing within-cluster loss
\item Imposes Voronoi partition where cluster boundaries are hyperplanes
\item Parameters are $k$, distance metric, scaling, initialization
\item Fails for non-convex shapes, unequal variance, noise
\end{itemize}
\item Hierarchical: 
\begin{itemize}
\item Reveal nested similarity structure through greedy merges or splits
\item No global geometric partition and shape depends on linkage choice 
\item Parameters are distance metric, linkage, cut height
\item Fails for noise, chaining, large $n$
\end{itemize}
\item DBSCAN: 
\begin{itemize}
\item Identify dense regions separated by low-density gaps and label the rest as noise
\item No partition of space and geometry adapts to data distribution
\item Parameters are $\varepsilon$, minPts, distance metric
\item Fails for varying densities, high dimension
\end{itemize}
\end{enumerate}
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Assessing cluster quality}
\addcontentsline{toc}{subsection}{Assessing cluster quality}

\begin{enumerate}
\item Fundamental difficulty:
\begin{enumerate}
\item Clustering is unsupervised: typically no ground truth labels, may not even be clusters in the first place.
\item "Good clustering" is not uniquely defined. Many metrics.
\item Evaluation criteria often encodes the same assumptions as the algorithm, can be deceptive.
\item Different metrics may rank the same clustering very differently.
\item Validation is about usefulness and stability, not correctness.
\end{enumerate}

\item Three perspectives on validation:
\begin{enumerate}
\item Internal validation: use only the data and clustering structure
\item External validation: compare to known labels (when available)
\item Relative validation: compare multiple clusterings to eachother
\end{enumerate}

\item Internal validation (geometry-based):
\begin{enumerate}
\item Measures compactness (within-cluster similarity) and separation (between-cluster dissimilarity).

\item Common quantities we've seen, algorithm specific:
\[
WCSS = \sum_{k=1}^K \sum_{i \in C_k} \|x_i - \mu_k \|^2 \quad \text{(within-cluster dispersion, built into kmeans)}
\]
\[
CCC = cor(d_{ij}, \hat{d}_{ij}) \quad \text{(cophenetic correlation, evaluates dendrogram vs data distance, hierarchial)}
\]

\item Calinski-Harabasz (CH) index: measures how well-separated clusters are relative to how compact they are.
\[
CH = \frac{SSB / (K-1)}{SSW / (n-K)} = \frac{\sum_k n_k d_2(\mu_k, \mu)^2 / (k-1)}{\sum_k \sum d_2(x_i, \mu_k)^2 / (n-k)}
\]
for $k$ the number of clusters and $n$ the sample size.
\begin{itemize}
\item Between-cluster variance $\rightarrow$ clusters should be far apart
\item Within-cluster variance $\rightarrow$ clusters should be tight
\item Higher CH the better
\end{itemize}

\item Silhouette score: measures how well each point fits within its assigned cluster compared to other clusters.
\begin{itemize}
\item For point $i$:
\[
a(i) = \text{ average distance to points in same cluster}
\]
\[
b(i) = \min_{k \neq c(i)} \text{ average distance to cluster $k$}
\]
\[
s(i) = \frac{b(i)-a(i)}{\max \{a(i), b(i) \}} \in [-1,1]
\]
\item $s \approx 1$: well-clustered
\item $s \approx 0$: ambiguous
\item $s < 0$: likely misclustered
\item Assumes distance-based, compact clusters, similar cluster density. Will penalize DBSCAN
\item Silhouette is an internal validation metric like $W$ and $B$, but it evaluates clustering locally at the point level rather than globally at the centroid level. Can average per cluster to get cluster metrics. Also average all for full clustering.
\end{itemize}
\end{enumerate}

\item Stability-based validation:
\begin{enumerate}
\item Idea: Good clusters should be reproducible under small perturbations.
\item Methods: Compare the data cluster solution to a new solution from...
\begin{itemize}
\item Subsampling the data
\item Adding noise
\item Bootstrap resampling
\end{itemize}
\item Compare cluster assignments across perturbations.
\begin{itemize}
\item Jaccard index
\[
J = \frac{|C_j \cap \hat{C}_j|}{|C_j \cup \hat{C}_j|}
\]
where $\hat{C}_j$ denotes the sampled / noise / bootstrap
\item Also adjusted Rand index (pairs of points in same cluster vs not adjusted for randomness).
\end{itemize}
\item Unstable clustering suggests:
\begin{itemize}
\item No strong structure
\item $K$ too large
\item Model overly sensitive to noise
\end{itemize}
\item Stability focuses on reliability rather than geometry.
\end{enumerate}


\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Readings}
\addcontentsline{toc}{subsection}{Readings}

\begin{enumerate}
\item DMCT Ch10, cluster analysis basic concepts and methods
\item DMCT 10.1, intro to cluster analysis
\item DMCT 10.2, partition methods (k-means and k-medoids)
\item ISL 12.4.1, k-means clustering
\item DMCT 10.3, hierarchical methods
\item ISL 12.4.2, hierarchical clustering 
\item DMCT 10.4.1, DBSCAN
\item ISL 12.4.3, practical issues in clustering
\item 
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Lab}
\addcontentsline{toc}{subsection}{lab}


\section*{Dimension reduction} 
\addcontentsline{toc}{section}{Dimension reduction}

Key motivations for dimension reduction: 
\begin{enumerate}
\item Fight the curse of dimensionality
\item Discover latent structure within data (key combinations of variables)
\item Remove noise dimensions from data
\item 2/3D visualization and model diagnostics
\end{enumerate}

Techniques we will see:
\begin{enumerate}
\item Singular value decomposition (SVD): Foundation for many modern computational algorithms, key framework for dimension reduction capturing dominant geometric directions
\item Principal component analysis (PCA): Decompose data into most statistically descriptive factors, follows from SVD
\item t-distributed stoichastic neighbor embedding (t-SNE): Nonlinear method focused on preserving local neighborhoods, cutting edge technique
\item Uniform manifold approximation and projection (UMAP): approximate manifold (surface) representation of data, cutting edge technique
\end{enumerate}

Demos:
\begin{enumerate}
\item \url{https://timbaumann.info/svd-image-compression-demo/}
\item \url{https://projector.tensorflow.org/}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Singular value decomposition}
\addcontentsline{toc}{subsection}{Singular value decomposition}

\begin{enumerate}
\item SVD idea: 
\begin{enumerate}
\item Determine a low-dimensional approximation of high-dimension data in terms of dominant patterns.
\item Example of a matrix factorization which works on any matrix $X_{n \times m}$. Solid linear algebra conversation.
\item Recap linear algebra and key factorizations: Matrix mult, $LU$, eigen decomposition $P D P^{-1}$, $QR$ decomposition
\end{enumerate}

\item SVD formation:
\begin{enumerate}
\item Data matrix $X = \left[ \vec{x}_1 \dots \vec{x}_m \right], \vec{x}_i \in \mathbb{R}^n$, though vectors could be complex valued.
\item Factorization:
\[
X_{n \times m} = U_{n \times n} \, \Sigma_{n \times m} \, V_{m \times m}^{T}
\]
where $U,V$ are unitary (orthonormal columns) contining left and right singular vectors (similar to eigenvectors) 
\[
U = \left[\vec{u}_1 \dots \vec{u}_n \right]_{n \times n}, \quad U^T U = U U^T = I
\]
\[
V = \left[\vec{v}_1 \dots \vec{v}_m \right]_{m \times m}
\]
and $\Sigma$ is a diagonal matrix of singular values (similar to eigenvalues)
\[
\Sigma =
\begin{bmatrix}
\sigma_1 & 0 & \cdots & 0 \\
0 & \sigma_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma_m \\
\hline
\multicolumn{4}{c}{\Large 0}
\end{bmatrix}
\]
where
\[
m = \operatorname{rank}(X), \quad
\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_m > 0.
\]
$\sigma_i$ is ordered by importance.
\item Note that $X_{n \times m}$ has $n >> m$, long rectangular matrix. 
\item The SVD always exists and is unique.
\item Because of the zeros in $\Sigma$, not all of $U$ is needed. This reduces to the so-called "economy" SVD.
\[
X = U \Sigma V^{T}
=
\begin{bmatrix}
\widehat{U} & \widehat{U}^{\perp}
\end{bmatrix}
\begin{bmatrix}
\widehat{\Sigma} \\
0
\end{bmatrix}
V^{T}
=
\widehat{U}\,\widehat{\Sigma}\,V^{T}.
\]
where $\widehat{U}_{m \times m}, \widehat{\Sigma}_{m \times m}$.
\end{enumerate}

\item SVD for matrix approximation:
\begin{enumerate}
\item 
\[
X = \left[\vec{x}_1 \vec{x}_2 \dots \vec{x}_m \right]
= \widehat{U}\,\widehat{\Sigma}\,V^{T}
=
\begin{bmatrix}
\vec{u}_1 ~ \vec{u}_2 ~ \dots ~ \vec{u}_m
\end{bmatrix}_{n \times m}
\begin{bmatrix}
\sigma_1 & 0 & \cdots & 0 \\
0 & \sigma_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma_m 
\end{bmatrix} _{m \times m}
\begin{bmatrix}
\vec{v}_1^T \\
\vec{v}_2^T \\
\vdots \\
\vec{v}_m^T
\end{bmatrix}_{m \times m}
\]
Do the multiplication.
\[
X = \sigma_1 \vec{u}_1 \vec{v_1}^T +  \sigma_2 \vec{u}_2 \vec{v_2}^T + \dots + \sigma_n \vec{u}_n \vec{v_n}^T
\]
This is the sum of $n$ rank 1 matrices (with unit columns, magnitude 1) ordered by contribution to matrix $X$.

\item Often trunkate at rank $r$ to get
\[
X \approx \tilde{U} \tilde{\Sigma} \tilde{V}^T
\]
noting that $\tilde{U}^T \tilde{U} = I_{r \times r}$ but $\tilde{U} \tilde{U}^T \neq I$.

\item The Eckard-Young theorem (1936) says this is the best possible rank $r$ approximation of $X$. 
\[
\arg\min_{\tilde{X} ~st~ rank(\tilde{X})=r} \| X - \tilde{X} \|_F =  \tilde{U} \tilde{\Sigma} \tilde{V}^T
\]
where the matrix Frobenius norm is
\[
\|A \|_F = \sqrt{\sum (A_{ij})^2}
\]
\end{enumerate}

\item Intuition behind SVD: Dominant correlations.
\begin{enumerate}
\item Data matrix where $n >> m$.
\[
X = \left[\vec{x}_1 ~ \vec{x_2} ~ \dots ~ \vec{x}_m \right]_{n \times m}
\] 
\item Correlation matrix:
\[
X^T X = \begin{bmatrix}
\vec{x}_1^T \\
\vec{x}_2^T \\
\vdots \\
\vec{x}_m^T 
\end{bmatrix}
\left[\vec{x}_1 ~ \vec{x_2} ~ \dots ~ \vec{x}_m \right]
= \begin{bmatrix}
\vec{x}_1^T \vec{x}_1 & \vec{x}_1^T \vec{x}_2 & \dots & \vec{x}_1^T \vec{x}_m \\
\vec{x}_2^T \vec{x}_1 & \vec{x}_2^T \vec{x}_2 & \dots & \vec{x}_2^T \vec{x}_m \\
\vdots & \vdots & \ddots & \vdots \\
\vec{x}_m^T \vec{x}_1 & \vec{x}_m^T \vec{x}_2 & \dots & \vec{x}_m^T \vec{x}_m
\end{bmatrix}
= \left[ \vec{x}_i^T \vec{x}_j\right]_{m \times m}
\]
This is a matrix of inner products of columns. Likewise $XX^T$ is the matrix of inner products of rows.
\item Assuming $X= \hat{U} \hat{\Sigma} V^T$ and so $X^T = V \hat{\Sigma}\hat{U}^T$, then
\[
X^T X = V \hat{\Sigma}^T \hat{U}^T \hat{U} \hat{\Sigma} V^T 
= V \hat{\Sigma}^2 V^T
\]
and so
\[
X^T X V = V \hat{\Sigma}^2
\]
and $V$ give the right eigenvectors with eigenvalues on the diagonal of $\hat{\Sigma}^2$. Likewise, for the left singular vectors:
\[
X X^T = \hat{U} \hat{\Sigma} V^T V \hat{\Sigma}^T \hat{U}^T = \hat{U} \hat{\Sigma}^2 \hat{U}^T
\]
and
\[
X X^T \hat{U} = \hat{U} \hat{\Sigma}^2
\]
so $\hat{U}$ gives the left eigenvectors of $X X^T$ with the same eigenvalues $\hat{\Sigma}^2$.

\item Full derivation:
\textbf{Short derivation of SVD:}

Let $X \in \mathbb{R}^{n \times m}$.

\begin{enumerate}
\item Compute $X^T X$, which is symmetric and positive semidefinite:
\[
X^T X \in \mathbb{R}^{m \times m}.
\]

\item Find eigenvectors $v_i$ and eigenvalues $\lambda_i \ge 0$ of $X^T X$:
\[
X^T X v_i = \lambda_i v_i, \quad i=1,\dots,r.
\]

\item Define singular values:
\[
\sigma_i = \sqrt{\lambda_i} \ge 0.
\]

\item Compute left singular vectors:
\[
u_i = \frac{X v_i}{\sigma_i} \in \mathbb{R}^n \quad \Rightarrow \quad X v_i = \sigma_i u_i.
\]

\item Assemble SVD:
\[
X = U \Sigma V^T
\]
where
\begin{itemize}
\item $U = [u_1, \dots, u_r] \in \mathbb{R}^{n \times r}$,
\item $V = [v_1, \dots, v_r] \in \mathbb{R}^{m \times r}$,
\item $\Sigma = \mathrm{diag}(\sigma_1, \dots, \sigma_r) \in \mathbb{R}^{r \times r}$,
\end{itemize}
and $r = \mathrm{rank}(X)$.
\end{enumerate} 
Extend $U$ and $V$ to orthonormal bases for $\mathbb{R}^n$ and $\mathbb{R}^m$ to get a full SVD.


\end{enumerate}
\end{enumerate}



\end{document}