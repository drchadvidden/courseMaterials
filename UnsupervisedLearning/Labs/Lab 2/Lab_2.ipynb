{"cells":[{"cell_type":"markdown","id":"5e73fbeb","metadata":{"id":"5e73fbeb"},"source":["# Lab 2: Distance and K-meand Clustering\n","\n","<a target=\"_blank\" href=\"https://raw.githubusercontent.com/drchadvidden/courseMaterials/refs/heads/main/UnsupervisedLearning/Labs/Lab%202/Lab_2.ipynb\">\n","<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>"]},{"cell_type":"markdown","source":["# Lab Instructions\n","\n","Run each of the coding cells. For tutorial example cells, understand the commands and check that the outputs make sense. For exercise cells, write your own code where indicated to generate the correct output. Give text explanations where indicated.\n","\n","### Submission:\n","Complete the following notebook in order. Once done, save the notebook, print the file as a .pdf, and upload the resulting file to the Canvas course assignment.\n","\n","### Rubric:\n","15 total points, 5 points to running tutorial example cells and saving outputs, 10 points for completing exercises.\n","\n","### Deadline:\n","Tuesday at midnight after the lab is assigned."],"metadata":{"id":"J5awbmuT7Ott"},"id":"J5awbmuT7Ott"},{"cell_type":"markdown","source":["# Tutorial: Distance and the curse of dimensionality"],"metadata":{"id":"MZOO-cKL83Qf"},"id":"MZOO-cKL83Qf"},{"cell_type":"markdown","source":["## Distance Metrics\n","\n","Let  \n","$$\n","\\mathbf{x} = (x_1, \\dots, x_d), \\quad \\mathbf{y} = (y_1, \\dots, y_d)\n","$$\n","be two vectors (data points) in $\\mathbb{R}^d$. Recall our three common distance metrics.\n","\n","Euclidean Distance ($\\ell_2$)\n","$$\n","d_2(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2}\n","$$\n","This is the “straight-line” distance and the default choice in many algorithms (e.g. k-means).\n","\n","Manhattan Distance ($\\ell_1$)\n","$$\n","d_1(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^d |x_i - y_i|\n","$$\n","Often used when movement is constrained along axes or when robustness to outliers is desired.\n","\n","Supremum Distance ($\\ell_{\\infty}$)\n","$$\n","d_\\infty(\\mathbf{x}, \\mathbf{y}) = \\max_{1 \\leq i \\leq d} |x_i - y_i |\n","$$\n","Measures the largest coordinate difference only, and ignores other differences."],"metadata":{"id":"RIshPdeyOJCR"},"id":"RIshPdeyOJCR"},{"cell_type":"code","source":["import numpy as np\n","\n","# this example code calculates distance 3 ways\n","x = np.array([2, 1, 5])\n","y = np.array([6, 4, 3])\n","\n","euclidean = np.linalg.norm(x - y, ord=2)\n","manhattan = np.linalg.norm(x - y, ord=1)\n","chebyshev = np.linalg.norm(x - y, ord=np.inf)\n","\n","print(f\"Euclidean distance:  {euclidean:.3f}\")\n","print(f\"Manhattan distance: {manhattan:.3f}\")\n","print(f\"Chebyshev distance: {chebyshev:.3f}\")"],"metadata":{"id":"sUQaWyI9SV1G"},"id":"sUQaWyI9SV1G","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## The Curse of Dimensionality\n","\n","As the number of dimensions increases, distances between points become less informative.\n","\n","Intuition:\n","*   In high dimensions, points tend to be far apart\n","*   The distance to the nearest neighbor becomes almost the same as the distance to the farthest neighbor\n","*   This degrades methods based on distance (k-NN, k-means, clustering) because nearness and farness is blurry\n","\n","To demonstrate, we generate random points in the unit hypercube $[0,1]^d$ ($d$-dimensional cube) and examine distances from a fixed reference point. As\n","$d$ increases, the difference between the minimum and maximum distance shrinks relative to their size. Points become almost equally distant.\n"],"metadata":{"id":"CtEs5BG9TstM"},"id":"CtEs5BG9TstM"},{"cell_type":"code","source":["import numpy as np\n","\n","def distance_stats(d, n=1000):\n","    x0 = np.random.rand(d) # random reference vector\n","    X = np.random.rand(n, d) # n random vectors to compare to reference vector\n","    dists = np.linalg.norm(X - x0, axis=1) #l_2 (Euclidean) distance\n","    return dists.min(), dists.max(), dists.mean(), x0, X\n","\n","dimensions = [2, 5, 10, 20, 50, 100, 500, 1000, 5000, 10000]\n","\n","# small dimension case\n","dmin, dmax, dmean, x0, X = distance_stats(2)\n","print(\"Example in small dimension:\")\n","print(x0)\n","print(X)\n","print(f\"d={d:3d} | min={dmin:.3f}, max={dmax:.3f}, mean={dmean:.3f}\")\n","\n","# summarize many dimension findings\n","print(\"\")\n","print(\"Example, summary stats as dimension increases\")\n","for d in dimensions:\n","    dmin, dmax, dmean, x0, X = distance_stats(d)\n","    print(f\"d={d:3d} | min={dmin:.3f}, max={dmax:.3f}, mean={dmean:.3f}\")\n"],"metadata":{"id":"yNVMAaHlUVAN"},"id":"yNVMAaHlUVAN","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tutorial: K-means"],"metadata":{"id":"JhVroSAGWYTI"},"id":"JhVroSAGWYTI"},{"cell_type":"markdown","source":["## Synthetic movie rating data\n","\n","The k-means clustering algorithm represents each cluster by its corresponding cluster centroid. The algorithm would partition the input data into k disjoint clusters by iteratively applying the following two steps:\n","\n","Form k clusters by assigning each instance to its nearest centroid.\n","Recompute the centroid of each cluster.\n","In this section, we perform k-means clustering on a toy example of movie ratings dataset. We first create the dataset as follows."],"metadata":{"id":"iC2rDgiViy2Z"},"id":"iC2rDgiViy2Z"},{"cell_type":"code","source":["import pandas as pd\n","\n","ratings = [\n","    ['john',   5, 5, 2, 1],\n","    ['mary',   4, 5, 3, 2],\n","    ['bob',    4, 4, 4, 3],\n","    ['lisa',   2, 2, 4, 5],\n","    ['lee',    1, 2, 3, 4],\n","    ['harry',  2, 1, 5, 5],\n","    ['alex',   5, 4, 2, 2],\n","    ['emma',   4, 5, 2, 1],\n","    ['chris',  5, 4, 3, 2],\n","    ['nina',   4, 4, 3, 2],\n","    ['paul',   2, 2, 5, 4],\n","    ['sara',   1, 2, 4, 5],\n","    ['mike',   2, 1, 4, 4],\n","    ['lily',   2, 2, 5, 5],\n","    ['drew',   3, 3, 3, 3],\n","    ['jordan', 3, 4, 4, 3],\n","]\n","\n","titles = ['user', 'Jaws', 'Star Wars', 'Exorcist', 'Saw']\n","movie_ratings = pd.DataFrame(ratings, columns=titles)\n","movie_ratings\n"],"metadata":{"id":"qWj9dhX7hitY"},"id":"qWj9dhX7hitY","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Read through this dataset and see if you can identify some preference patters of the users. Our goal is to apply k-means clustering on the users to identify groups of users with similar movie preferences.\n","\n","## K-means clustering (k=2)\n","\n","The example below shows how to apply k-means clustering (with k=2) on the movie ratings data. We must remove the \"user\" column first before applying the clustering algorithm. Feature scaling is not needed here because all ratings are on the same 1-5 scale. The cluster assignment for each user is displayed as a dataframe object."],"metadata":{"id":"Xe4AV3Khi5Yp"},"id":"Xe4AV3Khi5Yp"},{"cell_type":"code","source":["from sklearn import cluster\n","\n","data = movie_ratings.drop('user',axis=1)\n","k_means = cluster.KMeans(n_clusters=2, random_state=42, n_init=10)\n","k_means.fit(data)\n","labels = k_means.labels_\n","movie_ratings['Cluster ID'] = k_means.labels_\n","pd.DataFrame(labels, index=movie_ratings.user, columns=['Cluster ID'])"],"metadata":{"id":"X4BLcIfcjJtj"},"id":"X4BLcIfcjJtj","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Cluster centroids and naming\n","\n","Reading the table, we see the k-means clustering algorithm assigns the first three users to cluster 1 (action movie fans) and the next three users to cluster 0 (horror movie fans), and so on. The results are consistent with our expectation. We can also display the centroid for each of the two clusters. Not these centriods are cluster centers, not actual users."],"metadata":{"id":"LMdFCWOijXtP"},"id":"LMdFCWOijXtP"},{"cell_type":"code","source":["centroids = k_means.cluster_centers_\n","pd.DataFrame(centroids,columns=data.columns)"],"metadata":{"id":"YJHFWSlafZ7W"},"id":"YJHFWSlafZ7W","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Observe that cluster 0 has higher ratings for the horror movies whereas cluster 1 has higher ratings for action movies. It might be useful to rename these clusters in light of these observations."],"metadata":{"id":"C1ITS5vqjhZj"},"id":"C1ITS5vqjhZj"},{"cell_type":"code","source":["cluster_names = {\n","    0: 'Horror Fans',\n","    1: 'Action Fans'\n","}\n","movie_ratings['Cluster Name'] = movie_ratings['Cluster ID'].map(cluster_names)\n","movie_ratings\n"],"metadata":{"id":"pKcWVhHfnBFP"},"id":"pKcWVhHfnBFP","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cluster assignments can visually inspected. Feel free to check other movie comparisons."],"metadata":{"id":"aXVccv1EqFf0"},"id":"aXVccv1EqFf0"},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Select two movies for the x and y axes\n","x_col = 'Jaws'\n","y_col = 'Exorcist'\n","\n","# Map cluster names to colors\n","colors = {'Horror Fans': 'red', 'Action Fans': 'blue'}\n","user_colors = movie_ratings['Cluster Name'].map(colors)\n","\n","# Plot users\n","plt.scatter(movie_ratings[x_col], movie_ratings[y_col],\n","            c=user_colors, s=100, alpha=0.6, edgecolor='k')\n","\n","# Plot centroids\n","centroids_xy = k_means.cluster_centers_[:, [movie_ratings.columns.get_loc(x_col)-1,\n","                                            movie_ratings.columns.get_loc(y_col)-1]]\n","plt.scatter(centroids_xy[:,0], centroids_xy[:,1],\n","            c=['red','blue'], s=300, marker='X', edgecolor='k', linewidth=2)\n","\n","# Add legend\n","for name, color in colors.items():\n","    plt.scatter([], [], c=color, label=name)\n","plt.legend()\n","\n","plt.xlabel(x_col)\n","plt.ylabel(y_col)\n","plt.title(f'Movie Ratings Clusters: {x_col} vs {y_col}')\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"xnNGKWY1qK1a"},"id":"xnNGKWY1qK1a","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Cluster assignment for new data\n","\n","The cluster centroids can be applied to other users to determine their cluster assignments by simply finding the nearest centroid."],"metadata":{"id":"RoybupcrnIA7"},"id":"RoybupcrnIA7"},{"cell_type":"code","source":["import numpy as np\n","\n","testData = np.array([[4,5,1,2],[3,2,4,4],[2,3,4,1],[3,2,3,3],[5,4,1,4]])\n","labels = k_means.predict(testData)\n","labels = labels.reshape(-1,1)\n","usernames = np.array(['paul','kim','liz','tom','bill']).reshape(-1,1)\n","cols = movie_ratings.columns.tolist()\n","newusers = pd.DataFrame(\n","    data = np.column_stack((usernames, testData, labels)),\n","    columns = ['user', 'Jaws', 'Star Wars', 'Exorcist', 'Saw', 'Cluster ID']\n",")\n","newusers"],"metadata":{"id":"GPhM5FJ8jrlx"},"id":"GPhM5FJ8jrlx","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Elbow method for cluster identification\n","\n","To determine the number of clusters in the data, we can apply k-means with varying number of clusters from 1 to 6 and compute their corresponding within-cluster sum-of-squared (WCSS) errors as shown in the example below. The \"elbow\" in the plot of SSE versus number of clusters can be used to estimate the number of clusters. 2 clusters has a strong \"elbow\" indicating this is likely best, though maybe a 3 cluster solution is worth exploring."],"metadata":{"id":"qKVPB7btkGQS"},"id":"qKVPB7btkGQS"},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","numClusters = [1,2,3,4,5,6,7,8]\n","SSE = []\n","for k in numClusters:\n","    k_means = cluster.KMeans(n_clusters=k, random_state=42, n_init=10)\n","    k_means.fit(data)\n","    SSE.append(k_means.inertia_)\n","\n","plt.plot(numClusters, SSE)\n","plt.xlabel('Number of Clusters')\n","plt.ylabel('WCSS')"],"metadata":{"id":"LjcjX6hXkJX3"},"id":"LjcjX6hXkJX3","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CbUO3Bp_dBp6"},"id":"CbUO3Bp_dBp6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.cluster import KMeans\n","import matplotlib.pyplot as plt\n","\n","# Select the two columns\n","X = df[[\"Age\", \"Spending Score (1-100)\"]]\n","\n","# Fit k-means (choose k=3 as a reasonable default)\n","kmeans = KMeans(n_clusters=4, random_state=42)\n","df[\"Cluster\"] = kmeans.fit_predict(X)\n","\n","# Plot\n","plt.figure()\n","plt.scatter(\n","    df[\"Age\"],\n","    df[\"Spending Score (1-100)\"],\n","    c=df[\"Cluster\"]\n",")\n","\n","plt.xlabel(\"Age\")\n","plt.ylabel(\"Spending Score (1-100)\")\n","plt.title(\"Age vs Spending Score with K-Means Clusters\")\n","\n","plt.show()\n"],"metadata":{"id":"VomOwDpwhL-G"},"id":"VomOwDpwhL-G","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"bc7e71b4","metadata":{"id":"bc7e71b4"},"source":["# Exercise(s): Curse of dimensionality and k-means\n","\n","\n","\n"]},{"cell_type":"markdown","source":["## Exercise 1: Comparing Distance Metrics\n","\n","### Tasks:\n","\n","1. Generate random points in  $\\mathbb{R}^{10}$.\n","\n","2. Compute the following distances:\n","   - Euclidean distance\n","   - Manhattan distance\n","   - Supremum distance\n","\n","3. Repeat for $\\mathbb{R}^{100}$. Organize your results in nice table or visual.\n","\n","4. Compare how the distances change as the dimension increases. Which metric grows fastest with dimension? Which metric seems least sensitive to dimension?"],"metadata":{"id":"c6Jjkpv7_V0V"},"id":"c6Jjkpv7_V0V"},{"cell_type":"code","source":["# Write your code for the exercise 1 here!"],"metadata":{"id":"JfKK2B6tAsyP"},"id":"JfKK2B6tAsyP","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Explain your findings here:\n","\n","\n"],"metadata":{"id":"j9QclKNiAwKr"},"id":"j9QclKNiAwKr"},{"cell_type":"markdown","source":["## Exercise 2: Nearest vs Farthest Neighbor\n","\n","### Tasks\n","\n","1. Fix a random point $x_0 \\in [0,1]^d$\n","\n","2. Generate 1,000 random points in the same space.\n","\n","3. Compute all Euclidean distances to \\( x_0 \\).\n","\n","4. Calculate the ratio $\\frac{\\max(\\text{distance}) - \\min(\\text{distance})}{\\min(\\text{distance})}$\n","\n","5. Repeat for $d = 2,\\ 10,\\ 50,\\ 100...$. Organize your results in a nice table or visual. What happens to this ratio as the dimension increases? Why is this problematic for distance-based learning methods?\n"],"metadata":{"id":"Q1VJtj-bA60T"},"id":"Q1VJtj-bA60T"},{"cell_type":"code","source":["# Write your code for the exercise 2 here!"],"metadata":{"id":"ZGnfAKbAB8K2"},"id":"ZGnfAKbAB8K2","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Explain your findings here:\n"],"metadata":{"id":"YRtBrS_ZB-GI"},"id":"YRtBrS_ZB-GI"},{"cell_type":"markdown","source":["## Exercise 3: K-means on movie rating\n","\n","Use the above approach to find the $k=3$ clustering assignments.\n","\n","### Tasks:\n","\n","1. Assign cluster IDs to each user.\n","\n","2. Inspect the centroids and give them suitable names. Find the size of each cluster and add to the centroid descriptive table.\n","\n","3. Visualize your findings in 2D for pair(s) of movies.\n","\n","4. Find the average distance of users to their cluster centroid for each cluster. Compare findings for each cluster.\n","\n","5. Find the pairwise distance between each cluster."],"metadata":{"id":"tTzIFmmHrq6L"},"id":"tTzIFmmHrq6L"},{"cell_type":"code","source":[],"metadata":{"id":"2c-80eGrs16H"},"id":"2c-80eGrs16H","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Explain your findings here:\n"],"metadata":{"id":"DTw_TCYXswCX"},"id":"DTw_TCYXswCX"},{"cell_type":"markdown","source":["## Exercise 4: K-means on customer segmentation\n","\n","Explore a clustering of customer data.\n","\n","### Tasks:\n","\n","1. Process your data to decide which columns should be used (numeric only) and if normalization is needed.\n","\n","2. Create a WCSS elbow plot for many $k$-means clusterings and identify a \"best\" cluster number $k$ to explore.\n","\n","3. Assign cluster IDs to each user.\n","\n","4. Inspect the centroids and give them suitable names. Find the size of each cluster and add to the centroid descriptive table.\n","\n","5. Visualize your findings in 2D for pairs of customer attributes.\n","\n","6. Revert your normalization and summarize the centroids in terms of the original data scale.\n","\n","7. Analyze your clusters according to Gender. Do you see any relationships? Why does it make sense to not include the Gender feature when running $k$-means to create clusters in the first place?\n","\n","8. (Bonus) Analyze a second clustering solution and indicated in your elbow plot and compare to 3-6 results above."],"metadata":{"id":"MfciZn1jrz40"},"id":"MfciZn1jrz40"},{"cell_type":"code","source":["import pandas as pd\n","\n","# this file is also hosted on Kaggle: https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python\n","url = 'https://gist.githubusercontent.com/pravalliyaram/5c05f43d2351249927b8a3f3cc3e5ecf/raw/8bd6144a87988213693754baaa13fb204933282d/Mall_Customers.csv'\n","df = pd.read_csv(url)\n","\n","print(df.head())\n","print(df.info())\n","print(df.describe())\n","print(df[\"Gender\"].value_counts())"],"metadata":{"id":"YSv8f32R_esr"},"id":"YSv8f32R_esr","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Explain your findings here:\n"],"metadata":{"id":"wOZuFjCXs0Gq"},"id":"wOZuFjCXs0Gq"},{"cell_type":"markdown","source":["## Exercise 5: K-means exploration\n","\n","Summarize the ideas of this example in the Scikit-learn documentation: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py"],"metadata":{"id":"WT7taeXS5JrZ"},"id":"WT7taeXS5JrZ"},{"cell_type":"markdown","source":["### Explain your findings here:\n"],"metadata":{"id":"rcMMm6ua5DzJ"},"id":"rcMMm6ua5DzJ"}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"language_info":{"name":"python"},"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}