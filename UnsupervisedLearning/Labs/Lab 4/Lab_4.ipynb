{"cells":[{"cell_type":"markdown","id":"5e73fbeb","metadata":{"id":"5e73fbeb"},"source":["# Lab 4: Assessing Cluster Quality\n","\n","<a target=\"_blank\" href=\"https://colab.research.google.com/github/drchadvidden/courseMaterials/blob/main/UnsupervisedLearning/Labs/Lab%204/Lab_4.ipynb\">\n","<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>"]},{"cell_type":"markdown","source":["# Lab Instructions\n","\n","Run each of the coding cells. For tutorial example cells, understand the commands and check that the outputs make sense. For exercise cells, write your own code where indicated to generate the correct output. Give text explanations where indicated.\n","\n","### Submission:\n","Complete the following notebook in order. Once done, save the notebook, print the file as a .pdf, and upload the resulting file to the Canvas course assignment.\n","\n","### Rubric:\n","15 total points, 5 points to running tutorial example cells and saving outputs, 10 points for completing exercises.\n","\n","### Deadline:\n","Tuesday at midnight after the lab is assigned."],"metadata":{"id":"J5awbmuT7Ott"},"id":"J5awbmuT7Ott"},{"cell_type":"markdown","source":["# Tutorial: Cluster fit metrics"],"metadata":{"id":"MZOO-cKL83Qf"},"id":"MZOO-cKL83Qf"},{"cell_type":"markdown","source":["## Kmeans clustering of mall customer data\n","\n","Here we revisit the mall customer dataset from past homeworks."],"metadata":{"id":"RIshPdeyOJCR"},"id":"RIshPdeyOJCR"},{"cell_type":"code","source":["import pandas as pd\n","\n","# this file is also hosted on Kaggle: https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python\n","url = 'https://gist.githubusercontent.com/pravalliyaram/5c05f43d2351249927b8a3f3cc3e5ecf/raw/8bd6144a87988213693754baaa13fb204933282d/Mall_Customers.csv'\n","df = pd.read_csv(url)\n","\n","print(df.head())\n","print(df.info())\n","print(df.describe())\n","print(df[\"Gender\"].value_counts())"],"metadata":{"id":"sUQaWyI9SV1G"},"id":"sUQaWyI9SV1G","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Choosing the Number of Clusters (Model Selection)\n","\n","In this lab, we will perform K-means clustering using only two variables:\n","\n","- **Annual Income (k$)**\n","- **Spending Score (1–100)**\n","\n","This 2D setting makes it easy to visualize customer segments and interpret the results.\n","\n","Because K-means requires us to specify the number of clusters $ k $, we need a principled way to choose it. Rather than guessing, we will evaluate several values of $ k $ (from 2 to 10) using three common cluster selection criteria.\n","\n","---\n","\n","### 1️⃣ WCSS (Within-Cluster Sum of Squares)\n","\n","Also called **inertia**, this measures how tightly grouped the points are within each cluster.\n","\n","- Smaller values are better.\n","- We look for an **“elbow”** in the curve where improvement begins to level off.\n","\n","---\n","\n","### 2️⃣ Silhouette Score\n","\n","Measures how well each point fits within its cluster compared to other clusters.\n","\n","- Ranges from -1 to 1.\n","- Larger values indicate better separation.\n","- We typically choose the $ k $ that **maximizes** this score.\n","\n","---\n","\n","### 3️⃣ Calinski–Harabasz (CH) Index\n","\n","Measures the ratio of between-cluster separation to within-cluster cohesion.\n","\n","- Larger values indicate better-defined clusters.\n","- We choose the $ k $ that **maximizes** this index.\n","\n","---\n","\n","Before running K-means, we standardize the variables. Since K-means is distance-based, differences in scale would otherwise distort the clustering.\n","\n","The following code evaluates all three criteria for $ k = 2, \\dots, 10 $ and produces comparison plots.\n"],"metadata":{"id":"cyYP0kpGO-3Q"},"id":"cyYP0kpGO-3Q"},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score\n","\n","# ---------------------------\n","# 1. Select 2D Features\n","# ---------------------------\n","\n","X = df[[\"Annual Income (k$)\", \"Spending Score (1-100)\"]]\n","\n","# Standardize (important for distance-based clustering)\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# ---------------------------\n","# 2. Evaluate k = 2 to 10\n","# ---------------------------\n","\n","k_range = range(2, 11)\n","\n","wcss = []\n","silhouette_scores = []\n","ch_scores = []\n","\n","for k in k_range:\n","    kmeans = KMeans(n_clusters=k, random_state=42, n_init=20)\n","    labels = kmeans.fit_predict(X_scaled)\n","\n","    wcss.append(kmeans.inertia_)\n","    silhouette_scores.append(silhouette_score(X_scaled, labels))\n","    ch_scores.append(calinski_harabasz_score(X_scaled, labels))\n","\n","# ---------------------------\n","# 3. Plot Selection Criteria\n","# ---------------------------\n","\n","plt.figure()\n","plt.plot(k_range, wcss, marker='o')\n","plt.xlabel(\"k\")\n","plt.ylabel(\"WCSS\")\n","plt.title(\"Elbow Method\")\n","plt.show()\n","\n","plt.figure()\n","plt.plot(k_range, silhouette_scores, marker='o')\n","plt.xlabel(\"k\")\n","plt.ylabel(\"Silhouette Score\")\n","plt.title(\"Silhouette vs k\")\n","plt.show()\n","\n","plt.figure()\n","plt.plot(k_range, ch_scores, marker='o')\n","plt.xlabel(\"k\")\n","plt.ylabel(\"Calinski-Harabasz Index\")\n","plt.title(\"CH Index vs k\")\n","plt.show()"],"metadata":{"id":"D8FHDPsyPcWW"},"id":"D8FHDPsyPcWW","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fitting the Final K-Means Model\n","\n","Based on the previous model selection plots (WCSS, Silhouette, and CH index), we now choose an appropriate number of clusters $k$.\n","\n","Using this selected value, we:\n","\n","1. Fit the K-means model.\n","2. Assign each customer to a cluster.\n","3. Add the cluster labels to the dataframe.\n","4. Visualize the resulting segmentation in the 2D feature space.\n","\n","The scatterplot below shows how customers are grouped according to **Annual Income** and **Spending Score**, with colors indicating cluster membership.\n"],"metadata":{"id":"nXBoUGj2PpcI"},"id":"nXBoUGj2PpcI"},{"cell_type":"code","source":["k_optimal = 5\n","\n","kmeans = KMeans(n_clusters=k_optimal, random_state=42, n_init=20)\n","labels = kmeans.fit_predict(X_scaled)\n","\n","df[\"Cluster\"] = labels\n","\n","plt.figure()\n","\n","plt.scatter(\n","    df[\"Annual Income (k$)\"],\n","    df[\"Spending Score (1-100)\"],\n","    c=labels\n",")\n","\n","plt.xlabel(\"Annual Income (k$)\")\n","plt.ylabel(\"Spending Score (1-100)\")\n","plt.title(f\"K-Means Clustering (k={k_optimal})\")\n","\n","plt.show()\n"],"metadata":{"id":"AhnSA9obPpxW"},"id":"AhnSA9obPpxW","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tutorial: Cluster interpretation"],"metadata":{"id":"WTtXgs6O0sUv"},"id":"WTtXgs6O0sUv"},{"cell_type":"markdown","source":["## Summarizing Clusters for Interpretability\n","\n","Once we’ve assigned customers to clusters, the next step is to **summarize and interpret** those clusters. A key point in cluster-based segmentation is that the groups should not only be statistically distinct, they should also be **actionable and meaningful** to stakeholders — for example, marketing or product teams. Summary tables showing average characteristics per cluster help reveal whether the clusters differ in ways that make sense for targeting or strategy.\n","\n","This aligns with best practices in customer segmentation, which emphasize that segmentation analysis should go beyond a “black box” to provide **explorative and interpretable results** rather than just algorithm outputs (<https://www.researchgate.net/publication/30385490_A_Review_of_Unquestioned_Standards_in_Using_Cluster_Analysis_for_Data-Driven_Market_Segmentation>).\n"],"metadata":{"id":"GBUr6V7nReUP"},"id":"GBUr6V7nReUP"},{"cell_type":"code","source":["# ---------------------------\n","# Summarize Cluster Profiles\n","# ---------------------------\n","\n","# Select the variables we want to summarize\n","summary_vars = [\"Annual Income (k$)\", \"Spending Score (1-100)\", \"Age\"]\n","\n","# Compute mean and count per cluster\n","cluster_summary = df.groupby(\"Cluster\")[summary_vars].agg([\"count\", \"mean\", \"std\"])\n","print(cluster_summary)\n","\n","# Optionally show proportions by Gender\n","gender_dist = df.groupby(\"Cluster\")[\"Gender\"].value_counts(normalize=True).unstack()\n","print(gender_dist)\n"],"metadata":{"id":"5aZOpzifRhQY"},"id":"5aZOpzifRhQY","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Cluster Interpretation Table\n","\n","| Cluster | Income (k$) | Spending Score | Age | Gender | Interpretation |\n","|---------|------------|----------------|-----|--------|----------------|\n","| 0       | ~55        | ~43            | 50  | ~59% F | Middle-income, moderate spenders, older adults |\n","| 1       | ~87        | ~82            | 32  | ~54% F | High-income, high spenders, younger adults |\n","| 2       | ~26        | ~25            | 25  | ~59% F | Low-income, low spenders, young adults |\n","| 3       | ~88        | ~41            | 41  | ~46% F | High-income, moderate spenders, mid-age adults |\n","| 4       | ~26        | ~45            | 45  | ~61% F | Low-income, moderate spenders, mid-age adults |\n","\n","### Key Actionable Insights from Clusters\n","\n","- **High-income clusters (1 & 3)** could be targeted for premium or higher-end offers.  \n","- **Low-income clusters (2 & 4)** may respond better to budget-friendly campaigns.  \n","- **Gender distributions** are fairly balanced but could inform marketing messaging.  \n","- **Age differences** suggest using different channels or messaging strategies for different segments.  \n"],"metadata":{"id":"nBVFOVTxR4Vb"},"id":"nBVFOVTxR4Vb"},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Select variables to visualize\n","cluster_means = df.groupby(\"Cluster\")[[\"Annual Income (k$)\", \"Spending Score (1-100)\", \"Age\"]].mean()\n","\n","# Plot\n","cluster_means.plot(kind=\"bar\", figsize=(10,6))\n","plt.title(\"Average Characteristics per Cluster\")\n","plt.ylabel(\"Mean Value\")\n","plt.xticks(rotation=0)\n","plt.show()\n"],"metadata":{"id":"68Vg7Q_8R3-C"},"id":"68Vg7Q_8R3-C","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tutorial: Cluster stability"],"metadata":{"id":"tMqHbgGg0z80"},"id":"tMqHbgGg0z80"},{"cell_type":"markdown","source":["## Assessing Cluster Stability with Bootstrap\n","\n","Cluster assignments can vary if the data changes slightly. To check **stability**, we can use **bootstrap resampling**:\n","\n","1. **Bootstrap sampling**: Repeatedly draw random samples (with replacement) from the original data.\n","2. **Re-cluster** each sample using the same K-means parameters.\n","3. **Compare clusters**:\n","   - **Gap statistic**: Measures how much the clustering structure improves over a random uniform reference. Larger gap → more stable cluster separation.\n","   - **Adjusted Rand Index (ARI)**: Compares cluster assignments between bootstrap samples and the original clustering. ARI close to 1 → highly stable clusters.\n","\n","This approach helps validate that the chosen number of clusters $k$ produces **robust, reliable segments**, not just artifacts of a particular sample.\n"],"metadata":{"id":"25goZzU0Sb8K"},"id":"25goZzU0Sb8K"},{"cell_type":"code","source":["from sklearn.utils import resample\n","from sklearn.metrics import adjusted_rand_score\n","from sklearn.cluster import KMeans\n","import numpy as np\n","\n","k_optimal = 5\n","n_bootstrap = 50\n","ari_scores = []\n","\n","# Fit original clustering\n","kmeans_orig = KMeans(n_clusters=k_optimal, random_state=42, n_init=50)\n","labels_orig = kmeans_orig.fit_predict(X_scaled)\n","\n","# Bootstrap Loop\n","for i in range(n_bootstrap):\n","    # Resample the data (with replacement)\n","    X_resampled, indices = resample(X_scaled, np.arange(len(X_scaled)), random_state=i)\n","\n","    # Fit KMeans on the bootstrap sample\n","    kmeans_boot = KMeans(n_clusters=k_optimal, random_state=i, n_init=10)\n","    labels_boot = kmeans_boot.fit_predict(X_resampled)\n","\n","    # Compare original labels to bootstrap labels for the sampled indices\n","    ari = adjusted_rand_score(labels_orig[indices], labels_boot)\n","    ari_scores.append(ari)\n","\n","# Quick stats\n","print(f\"Mean ARI: {np.mean(ari_scores):.4f} (+/- {np.std(ari_scores):.4f})\")\n","\n","import matplotlib.pyplot as plt\n","\n","# Simple line plot of all bootstrap scores\n","plt.plot(ari_scores, marker='o', linestyle='-')\n","plt.axhline(sum(ari_scores)/len(ari_scores), color='red', label='Mean')\n","\n","plt.title('ARI Score per Bootstrap Iteration')\n","plt.xlabel('Iteration')\n","plt.ylabel('ARI Score')\n","plt.legend()\n","plt.show()\n","\n"],"metadata":{"id":"lie7CMPdSiJC"},"id":"lie7CMPdSiJC","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Next Steps Toward a More Robust Customer Segmentation\n","\n","The current clustering analysis provides an initial segmentation based on income and spending behavior. However, a robust customer segmentation strategy typically goes beyond a single K-means model on two variables.\n","\n","Potential next steps include:\n","\n","- **Incorporate additional features** (e.g., age, gender, purchase frequency, product categories) to capture richer behavioral patterns.\n","- **Compare alternative clustering methods** (e.g., hierarchical clustering or DBSCAN models) to assess whether segment structure is consistent.\n","- **Evaluate stability over time**, if longitudinal data are available.\n","- **Validate business usefulness**, ensuring segments are actionable for marketing, pricing, or personalization strategies.\n","- **Profile and label segments clearly**, translating statistical clusters into meaningful customer personas.\n","\n","Effective segmentation balances statistical validity, stability, and managerial interpretability.\n"],"metadata":{"id":"KrqcEMGy0Qhj"},"id":"KrqcEMGy0Qhj"},{"cell_type":"markdown","id":"bc7e71b4","metadata":{"id":"bc7e71b4"},"source":["# Exercise(s): Cluster assessment and validity\n","\n","\n","\n"]},{"cell_type":"markdown","source":["## Exercise 1: Silhouette score exploration\n","\n","Cluster evaluation need not rely only on a single overall silhouette score.  \n","A strong average value can sometimes hide poorly separated clusters or misclassified observations.  \n","In this exercise, you will examine silhouette scores at the **individual observation level** to better understand cluster structure and separation.\n","\n","\n","### Tasks:\n","1. **Compute silhouette values for each observation**  \n","   - Use `silhouette_samples()` from `sklearn.metrics`.  \n","   - Store the result in a new column in your dataframe.\n","\n","2. **Compute the average silhouette score per cluster**  \n","   - Group by cluster label.  \n","   - Report the mean and standard deviation silhouette score for each cluster.  \n","\n","3. **Create a visualization**  \n","   - Produce a plot that shows silhouette values grouped by cluster.  \n","   - A boxplot or bar chart of average silhouette scores is sufficient, but you could color points on the clustering scatterplot with clusters by shapes.  \n","   - Clearly label axes and clusters.\n","\n","4. **Interpretation Questions**\n","   - Which cluster has the **lowest average silhouette score**?\n","   - Are there any **negative silhouette values**?\n","   - What do negative silhouette values indicate?\n","   - Do any clusters appear poorly separated or internally inconsistent?\n","   - Would this change your confidence in the chosen value of $k$?"],"metadata":{"id":"c6Jjkpv7_V0V"},"id":"c6Jjkpv7_V0V"},{"cell_type":"code","source":["# Write your code for the exercise here!"],"metadata":{"id":"JfKK2B6tAsyP"},"id":"JfKK2B6tAsyP","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Explain your findings here:\n","\n","\n"],"metadata":{"id":"j9QclKNiAwKr"},"id":"j9QclKNiAwKr"},{"cell_type":"markdown","source":["## Exercise 2: Comparing $k=4$, $k=5$, and $k=6$ — Fit, Stability, and Interpretability\n","\n","In the tutorial, we selected $k=5$ based on WCSS, silhouette, and CH index.  \n","However, cluster analysis rarely has a single “correct” solution. Different values of $k$ may provide different trade-offs between statistical fit, stability, and managerial usefulness.\n","\n","In this exercise, you will critically evaluate whether $k=5$ is truly preferable by comparing it to $k=4$ and $k=6$.\n","\n","---\n","\n","## Part A — Model Fit\n","\n","For each value of $k \\in \\{4,5,6\\}$:\n","\n","1. Fit a K-means model using the same scaled features.\n","2. Compute:\n","   - Overall silhouette score\n","   - Calinski–Harabasz (CH) index\n","3. Compare the three models:\n","   - Which $k$ has the highest silhouette?\n","   - Which has the highest CH index?\n","   - Are differences large or marginal?\n","\n","### Considerations\n","- Does statistical fit clearly favor one model?\n","- If the metrics disagree, how would you decide?\n","\n","---\n","\n","## Part B — Stability (Bootstrap ARI)\n","\n","For each $k$:\n","\n","1. Perform bootstrap resampling.\n","2. Compute the Adjusted Rand Index (ARI) between the original clustering and bootstrap clusterings.\n","3. Report the **mean ARI** across bootstrap samples.\n","\n","### Considerations\n","- Which $k$ appears most stable?\n","- Does the most stable solution match the best silhouette score?\n","- What does low stability imply about segmentation reliability?\n","\n","---\n","\n","## Part C — Segment Structure and Interpretability\n","\n","For each $k$:\n","\n","1. Summarize cluster sizes.\n","2. Profile clusters using mean:\n","   - Income\n","   - Spending\n","   - Age\n","   - Gender\n","3. Compare the segmentation structure:\n","   - Does $k=4$ merge meaningful groups?\n","   - Does $k=6$ create very small or fragmented clusters?\n","   - Does $k=5$ provide a clearer business narrative?\n","\n","### Considerations\n","\n","- Which solution produces the most **balanced cluster sizes**?\n","- Which produces the most **distinct behavioral profiles**?\n","- Are any clusters difficult to describe in marketing terms?\n","- Does increasing $k$ meaningfully improve insight, or just add complexity?\n","\n","---\n","\n","## Final Deliverable\n","\n","Write a short paragraph (5–8 sentences) recommending one value of $k$.\n","\n","Your justification must reference:\n","- Statistical fit (silhouette / CH)\n","- Stability (ARI)\n","- Cluster size balance\n","- Business interpretability\n"],"metadata":{"id":"FPZb7kmdF8qU"},"id":"FPZb7kmdF8qU"},{"cell_type":"code","source":["# Write your code for the exercise here!"],"metadata":{"id":"QwhtyutOF8D_"},"execution_count":null,"outputs":[],"id":"QwhtyutOF8D_"},{"cell_type":"markdown","source":["### Explain your findings here:\n","\n","\n"],"metadata":{"id":"-Ot_81X0F5Tq"},"id":"-Ot_81X0F5Tq"},{"cell_type":"markdown","source":["## Exercise 3: Domain Expertise\n","\n","The tutorial cited the following paper:\n","\n","<https://www.researchgate.net/publication/30385490_A_Review_of_Unquestioned_Standards_in_Using_Cluster_Analysis_for_Data-Driven_Market_Segmentation>\n","\n","This paper argues that cluster analysis in marketing is often applied mechanically, without sufficient attention to business relevance and validation.\n","\n","---\n","\n","### High-Level Takeaways\n","\n","1. **Statistical fit is not enough**  \n","   Good silhouette or internal validity metrics do not guarantee meaningful or useful market segments.\n","\n","2. **Segments must be actionable**  \n","   Clusters should lead to clear managerial decisions (targeting, positioning, pricing, communication).\n","\n","3. **Stability and robustness matter**  \n","   Segments should not change dramatically with small data perturbations.\n","\n","4. **Interpretability is critical**  \n","   If a cluster cannot be clearly described in business terms, it is unlikely to be useful.\n","\n","---\n","\n","### Short Reflection\n","\n","In 4–6 sentences, discuss whether your chosen clustering solution satisfies these broader criteria beyond statistical metrics.\n"],"metadata":{"id":"WhvBaJf1GkN6"},"id":"WhvBaJf1GkN6"},{"cell_type":"markdown","source":["### Explain your findings here:\n","\n","\n"],"metadata":{"id":"wKiXq4gRGh3y"},"id":"wKiXq4gRGh3y"},{"cell_type":"markdown","source":["## Exercise 4: Wholesale Customer Segmentation\n","\n","In this final exercise, you will apply the full clustering workflow to a new dataset:  \n","the **Wholesale Customers Dataset** from the UCI Machine Learning Repository.\n","\n","Unlike the mall dataset, this dataset contains annual spending across multiple product categories. Your goal is to construct meaningful customer segments and evaluate their statistical and managerial quality.\n","\n","---\n","\n","### Tasks\n","\n","1. **Load and Explore the Data**\n","   - Load the dataset from the provided online source.\n","   - Examine summary statistics.\n","   - Identify the feature columns to use for clustering.\n","   - Decide whether any variables should be excluded.\n","\n","2. **Preprocess the Data**\n","   - Standardize the numeric features.\n","   - Briefly justify why scaling is necessary.\n","\n","3. **Select the Number of Clusters**\n","   - Evaluate $k = 2$ to $k = 10$.\n","   - Compute:\n","     - WCSS (Elbow Method)\n","     - Silhouette Score\n","     - Calinski–Harabasz Index\n","   - Select an optimal $k$ and justify your choice.\n","\n","4. **Assess Cluster Stability**\n","   - Perform bootstrap resampling.\n","   - Compute the mean Adjusted Rand Index (ARI).\n","   - Briefly interpret the stability of your solution.\n","\n","5. **Profile and Interpret Clusters**\n","   - Summarize cluster means for each spending category.\n","   - Identify distinguishing characteristics of each segment.\n","   - Provide descriptive labels for each cluster.\n","\n","6. **Business Interpretation**\n","   - Propose at least two actionable strategies based on your segments.\n","   - Discuss whether your segmentation is statistically strong, stable, and managerially useful.\n","\n","---\n","\n","### Deliverables\n","\n","- One figure for model selection  \n","- One table summarizing cluster profiles  \n","- One stability metric (mean ARI)  \n","- A short written interpretation (8–12 sentences)\n","\n"],"metadata":{"id":"jT3gNGnIH06b"},"id":"jT3gNGnIH06b"},{"cell_type":"code","source":["# Write your code for the exercise here!\n","\n","import pandas as pd\n","\n","df_wholesale = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv\")\n","df_wholesale.head()"],"metadata":{"id":"kN-E0qyuGgKg"},"execution_count":null,"outputs":[],"id":"kN-E0qyuGgKg"},{"cell_type":"markdown","source":["### Explain your findings here:\n","\n","\n"],"metadata":{"id":"AYDN6xuIH1LF"},"id":"AYDN6xuIH1LF"},{"cell_type":"code","source":["# code to export notebook as .html for Canvas upload\n","\n","from google.colab import drive\n","from google.colab import files\n","\n","drive.mount('/content/drive')\n","\n","notebook_name = \"Lab_3\"\n","!cp \"/content/drive/MyDrive/Colab Notebooks/DSC 430/{notebook_name}.ipynb\" /content/\n","!jupyter nbconvert --to html \"/content/{notebook_name}.ipynb\"\n","files.download(f\"/content/{notebook_name}.html\")\n","\n"],"metadata":{"id":"Abfe5R4PKF2D"},"id":"Abfe5R4PKF2D","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VhVEOcNoIr-W"},"id":"VhVEOcNoIr-W","execution_count":null,"outputs":[]}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}